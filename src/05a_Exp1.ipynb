{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1a76c7e24334171ac7c2d09bf67d66b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c167d5ed4204faa80ae6b9201fac462",
              "IPY_MODEL_f3f112b765d54c34af56dace949a7193",
              "IPY_MODEL_021370974eb7400ca2633e063dbe9ae2"
            ],
            "layout": "IPY_MODEL_76cdf49db13043eea95d61c744e2e121"
          }
        },
        "2c167d5ed4204faa80ae6b9201fac462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_723bacadcb4c4e6c9fe9cbeaa3f33ecd",
            "placeholder": "​",
            "style": "IPY_MODEL_93e692d8e1b44974bebe60685da98a12",
            "value": "onnx/model.onnx: 100%"
          }
        },
        "f3f112b765d54c34af56dace949a7193": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a55707125fa4cddb06a068f0eb87acc",
            "max": 1262681582,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d3963bbd1e143a8ac154daa73b05a21",
            "value": 1262681582
          }
        },
        "021370974eb7400ca2633e063dbe9ae2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8615d9d5de4045cbb25f74c42d3cbd57",
            "placeholder": "​",
            "style": "IPY_MODEL_b9c9ade2b95b43e38f3ad18e2e4564ea",
            "value": " 1.26G/1.26G [00:17&lt;00:00, 169MB/s]"
          }
        },
        "76cdf49db13043eea95d61c744e2e121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "723bacadcb4c4e6c9fe9cbeaa3f33ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93e692d8e1b44974bebe60685da98a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a55707125fa4cddb06a068f0eb87acc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d3963bbd1e143a8ac154daa73b05a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8615d9d5de4045cbb25f74c42d3cbd57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9c9ade2b95b43e38f3ad18e2e4564ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c5d0dbee59e4b16a93a12efda3b93da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_3c71c16389fe4525a8b6ab7c51369f93"
          }
        },
        "bea2f9c1ba324322b2160f3be69ce5d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f6113e9f2a342e789db7b9c6bd230a9",
            "placeholder": "​",
            "style": "IPY_MODEL_c772049bff394449b8ec704befe829c7",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "d76fcd4b14df4ca7ba2e2a8032a59693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4cd089bdaf5e428d819ac5362ad44109",
            "placeholder": "​",
            "style": "IPY_MODEL_73b11b4febc342e09a05e304404fed1f",
            "value": ""
          }
        },
        "52b31945523245e7a303f5585c247fd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_9165c3c74e264c78817a1d1f59ac6bbb",
            "style": "IPY_MODEL_decf86e6afbb4808b9c98b1a335fcc0c",
            "value": true
          }
        },
        "746439369cb147158b46bbd3127db2cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d2a4b414d5324e2b99cde78adee24de4",
            "style": "IPY_MODEL_cf620670c4e4473e99d732e66d4de4f0",
            "tooltip": ""
          }
        },
        "0f9ed689ca3e4dc5871161267cbd06a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f1e0d7a9bd548418b5a4741d3ae6f90",
            "placeholder": "​",
            "style": "IPY_MODEL_2ea6c0db0d0e4de8ba3610504afc1142",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "3c71c16389fe4525a8b6ab7c51369f93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "2f6113e9f2a342e789db7b9c6bd230a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c772049bff394449b8ec704befe829c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cd089bdaf5e428d819ac5362ad44109": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73b11b4febc342e09a05e304404fed1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9165c3c74e264c78817a1d1f59ac6bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "decf86e6afbb4808b9c98b1a335fcc0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d2a4b414d5324e2b99cde78adee24de4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf620670c4e4473e99d732e66d4de4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "6f1e0d7a9bd548418b5a4741d3ae6f90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ea6c0db0d0e4de8ba3610504afc1142": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "037d19baa8e64708b9d07f53aadd6e96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef8fb347c8e049a0918e6793704c6ccb",
            "placeholder": "​",
            "style": "IPY_MODEL_e53eae35687d4b4c94875a04f98a2dde",
            "value": "Connecting..."
          }
        },
        "ef8fb347c8e049a0918e6793704c6ccb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e53eae35687d4b4c94875a04f98a2dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bea67f326134ae699c168439181dffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5f4c435a98f4e81b31ae629a65ac36a",
              "IPY_MODEL_3ae60ea516dd4092acd685ede7b4ab1e",
              "IPY_MODEL_62ad0d6ccd794e6da0c474bf3e9f637f"
            ],
            "layout": "IPY_MODEL_9f93e80c525f4e279815e6e9fb9641e4"
          }
        },
        "c5f4c435a98f4e81b31ae629a65ac36a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d4dd78af1140c6b52f708f17e6cf53",
            "placeholder": "​",
            "style": "IPY_MODEL_3cc042f62f19478fb40a42cee72d87be",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "3ae60ea516dd4092acd685ede7b4ab1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b264925afcf4f56aa823b27fc35d3a1",
            "max": 260,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9150a51f7e5a49c48ffe2dbf9b571450",
            "value": 260
          }
        },
        "62ad0d6ccd794e6da0c474bf3e9f637f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01bfc21a5ef642e78edcb9363e3f13d1",
            "placeholder": "​",
            "style": "IPY_MODEL_9252ec5c6cf5412e9a4fe714ef488278",
            "value": " 260/260 [00:00&lt;00:00, 7.71kB/s]"
          }
        },
        "9f93e80c525f4e279815e6e9fb9641e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1d4dd78af1140c6b52f708f17e6cf53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cc042f62f19478fb40a42cee72d87be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b264925afcf4f56aa823b27fc35d3a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9150a51f7e5a49c48ffe2dbf9b571450": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01bfc21a5ef642e78edcb9363e3f13d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9252ec5c6cf5412e9a4fe714ef488278": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71c95d777d1d4fff8b17bd1011452945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95349f6af5b245988d37fc43aa7a5e51",
              "IPY_MODEL_1fcc786967f04607b3b9d0cd1befa96f",
              "IPY_MODEL_1509b7c452b645baa4fbf66dfabd50cd"
            ],
            "layout": "IPY_MODEL_4d3902c5bd8548198f6bd21c3934c6cc"
          }
        },
        "95349f6af5b245988d37fc43aa7a5e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4184ce3ff5f4d1996604158b571ca6f",
            "placeholder": "​",
            "style": "IPY_MODEL_485519f1c00743db933590fcb19ca8fb",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "1fcc786967f04607b3b9d0cd1befa96f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b47d4403095f4999a0f5f4aee1df5897",
            "max": 257,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9665e115e1dd470c85b7c54ab727305f",
            "value": 257
          }
        },
        "1509b7c452b645baa4fbf66dfabd50cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30c688cbbded4187bb9532fc936c7efe",
            "placeholder": "​",
            "style": "IPY_MODEL_29e5307166c4445f8b76c20e53ac98b9",
            "value": " 257/257 [00:00&lt;00:00, 10.8kB/s]"
          }
        },
        "4d3902c5bd8548198f6bd21c3934c6cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4184ce3ff5f4d1996604158b571ca6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "485519f1c00743db933590fcb19ca8fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b47d4403095f4999a0f5f4aee1df5897": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9665e115e1dd470c85b7c54ab727305f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "30c688cbbded4187bb9532fc936c7efe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29e5307166c4445f8b76c20e53ac98b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66b36b8afdd84c8793a0064166ac748f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1776b82a20094df098816ab39197a5da",
              "IPY_MODEL_ae0b2bdd5a96432a86eaec11ba134671",
              "IPY_MODEL_06dd1168f30d47e78eceb8e689f0d4be"
            ],
            "layout": "IPY_MODEL_8bc5356225d743589258581cb6e8b85a"
          }
        },
        "1776b82a20094df098816ab39197a5da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_572b990f2bb84291bbbee010108c02f3",
            "placeholder": "​",
            "style": "IPY_MODEL_798c9f743dc145e9b7c7d027834b51d5",
            "value": "vocab.json: 100%"
          }
        },
        "ae0b2bdd5a96432a86eaec11ba134671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f6f3588fe24893a1eb7cc40f16f10a",
            "max": 741,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_120a67efdb584492bbd47b5f337eeb1c",
            "value": 741
          }
        },
        "06dd1168f30d47e78eceb8e689f0d4be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_828a635f89a9451c8c358b65168210c5",
            "placeholder": "​",
            "style": "IPY_MODEL_f82c7763ab2b49d0889ccb0446afeb82",
            "value": " 741/741 [00:00&lt;00:00, 19.9kB/s]"
          }
        },
        "8bc5356225d743589258581cb6e8b85a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "572b990f2bb84291bbbee010108c02f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "798c9f743dc145e9b7c7d027834b51d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65f6f3588fe24893a1eb7cc40f16f10a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "120a67efdb584492bbd47b5f337eeb1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "828a635f89a9451c8c358b65168210c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f82c7763ab2b49d0889ccb0446afeb82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0007ce5c346483ea0bed61643a0be33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1346ac69b4034a93bcd6f5d9a73f0221",
              "IPY_MODEL_8f10180623a94ce8b5f5da5b253103f0",
              "IPY_MODEL_b61c4f5a32cc4022bfaca04436d21034"
            ],
            "layout": "IPY_MODEL_5c9e14c332c649d7880ce921814b0f23"
          }
        },
        "1346ac69b4034a93bcd6f5d9a73f0221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d0db13c177c44fc8531b7ab651fb220",
            "placeholder": "​",
            "style": "IPY_MODEL_6e077998a8a14773be1594ee515c234c",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "8f10180623a94ce8b5f5da5b253103f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a97e01a020604bf988389a9353fc9fcf",
            "max": 85,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6cfabd373bd4b00a13a67c3fa2161ad",
            "value": 85
          }
        },
        "b61c4f5a32cc4022bfaca04436d21034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d939681e39470da93fd56941f09345",
            "placeholder": "​",
            "style": "IPY_MODEL_a5c561e3a5d140e98fdd8c2891779ce3",
            "value": " 85.0/85.0 [00:00&lt;00:00, 1.99kB/s]"
          }
        },
        "5c9e14c332c649d7880ce921814b0f23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d0db13c177c44fc8531b7ab651fb220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e077998a8a14773be1594ee515c234c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a97e01a020604bf988389a9353fc9fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6cfabd373bd4b00a13a67c3fa2161ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40d939681e39470da93fd56941f09345": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c561e3a5d140e98fdd8c2891779ce3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82adc37fed4d4df9abaf74546a92f296": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_235c09f91368425dbbd9e681771af649",
              "IPY_MODEL_c6f90ad002134686a2a5c1e174b0e3e2",
              "IPY_MODEL_0fa0a66489414370bb9ae2cece4b6892"
            ],
            "layout": "IPY_MODEL_3994bfb7f52e4bdc85b036374fdc49f1"
          }
        },
        "235c09f91368425dbbd9e681771af649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88a5f94919f54bcea2f6f0a404a04999",
            "placeholder": "​",
            "style": "IPY_MODEL_d553b5ae4b4c44469d49ba0740acb4bd",
            "value": "config.json: 100%"
          }
        },
        "c6f90ad002134686a2a5c1e174b0e3e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39f396ead69b4ce08d55c4d1d4a3e1f9",
            "max": 2100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bdd0a0d9142747c4b606a87e9143f1a3",
            "value": 2100
          }
        },
        "0fa0a66489414370bb9ae2cece4b6892": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef26f1508f4c4df8a08b3088dc12d738",
            "placeholder": "​",
            "style": "IPY_MODEL_d297b8c8270b44baa5f3045b8cf16c48",
            "value": " 2.10k/2.10k [00:00&lt;00:00, 148kB/s]"
          }
        },
        "3994bfb7f52e4bdc85b036374fdc49f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88a5f94919f54bcea2f6f0a404a04999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d553b5ae4b4c44469d49ba0740acb4bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "39f396ead69b4ce08d55c4d1d4a3e1f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdd0a0d9142747c4b606a87e9143f1a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef26f1508f4c4df8a08b3088dc12d738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d297b8c8270b44baa5f3045b8cf16c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45d42cd3bea1442180e4f9501e26a2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d132c36ee34b47508ae9c0c3bc9cd8e8",
              "IPY_MODEL_4b4f8a74dc9d489eb3b49e973179c57f",
              "IPY_MODEL_ec45338ed9fb49c48406ae0fc2f0ee2a"
            ],
            "layout": "IPY_MODEL_7a9fcea100814249ade7360eb944e2d7"
          }
        },
        "d132c36ee34b47508ae9c0c3bc9cd8e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2413b7e865874adb98e4fc8e40578d26",
            "placeholder": "​",
            "style": "IPY_MODEL_bb9121e647314a3f85f095e7fd7814df",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "4b4f8a74dc9d489eb3b49e973179c57f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c61a4023a964b7983360a2a29f076fe",
            "max": 1262181719,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a3c0e0c14dc468ea3857f9705917b2c",
            "value": 1262181719
          }
        },
        "ec45338ed9fb49c48406ae0fc2f0ee2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90626c86f9a94ca49a563450067b4da3",
            "placeholder": "​",
            "style": "IPY_MODEL_00d979a90a954dc2ac8e3a88c019d0f0",
            "value": " 1.26G/1.26G [00:18&lt;00:00, 99.7MB/s]"
          }
        },
        "7a9fcea100814249ade7360eb944e2d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2413b7e865874adb98e4fc8e40578d26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb9121e647314a3f85f095e7fd7814df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8c61a4023a964b7983360a2a29f076fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a3c0e0c14dc468ea3857f9705917b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "90626c86f9a94ca49a563450067b4da3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00d979a90a954dc2ac8e3a88c019d0f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7cdc22d026a46c59de5ceeda7396333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9b63a6ee23d046ea9500462ceaff4cf7",
              "IPY_MODEL_57c923eefe9b495cac534ba3ed6d4e8b",
              "IPY_MODEL_d6de7a9ebf5840fea773b59954f3cd8a"
            ],
            "layout": "IPY_MODEL_0eb7c6608f8d468c9bd7cf938e3e165c"
          }
        },
        "9b63a6ee23d046ea9500462ceaff4cf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db4d30cad366463596262252de9d1c90",
            "placeholder": "​",
            "style": "IPY_MODEL_d4e89f93fff34034b3f2d883da44892b",
            "value": "model.safetensors: 100%"
          }
        },
        "57c923eefe9b495cac534ba3ed6d4e8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_121c77289981414faf36a6cd69b4bef2",
            "max": 1262086232,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83f71ee1296b4ba186ab966a065e4929",
            "value": 1262086232
          }
        },
        "d6de7a9ebf5840fea773b59954f3cd8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4ed62f7131a41a190088e3c4524bc01",
            "placeholder": "​",
            "style": "IPY_MODEL_8e8690141b5d48968830a5e26118c651",
            "value": " 1.26G/1.26G [00:19&lt;00:00, 141MB/s]"
          }
        },
        "0eb7c6608f8d468c9bd7cf938e3e165c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db4d30cad366463596262252de9d1c90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4e89f93fff34034b3f2d883da44892b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "121c77289981414faf36a6cd69b4bef2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f71ee1296b4ba186ab966a065e4929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4ed62f7131a41a190088e3c4524bc01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e8690141b5d48968830a5e26118c651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c0738358416549fa88d1efcd3d112200": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1beff33c3a3f4d0499c0fd11150b15f8",
              "IPY_MODEL_12133294e0f2461bbdaa5a049ccc8ee8",
              "IPY_MODEL_390276c5bd9d43a7ac3af3d8dfbb855c"
            ],
            "layout": "IPY_MODEL_486f5651dbb9444597919ad2927c69b2"
          }
        },
        "1beff33c3a3f4d0499c0fd11150b15f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ae1aaf9daa243c381e0bb5cf8a247be",
            "placeholder": "​",
            "style": "IPY_MODEL_0534285e54c446518b267b19ad3da176",
            "value": "README.md: 100%"
          }
        },
        "12133294e0f2461bbdaa5a049ccc8ee8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_402ee573140142a6bbac8feb15422e7a",
            "max": 436,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_946fa54f54a24f9789e6d7770797e39a",
            "value": 436
          }
        },
        "390276c5bd9d43a7ac3af3d8dfbb855c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_606c9266a180450daf47e4c38f352e7b",
            "placeholder": "​",
            "style": "IPY_MODEL_470f887c2b404d05b079c71b9fbe4e6e",
            "value": " 436/436 [00:00&lt;00:00, 13.9kB/s]"
          }
        },
        "486f5651dbb9444597919ad2927c69b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ae1aaf9daa243c381e0bb5cf8a247be": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0534285e54c446518b267b19ad3da176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "402ee573140142a6bbac8feb15422e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "946fa54f54a24f9789e6d7770797e39a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "606c9266a180450daf47e4c38f352e7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "470f887c2b404d05b079c71b9fbe4e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VcZklhIov8a",
        "outputId": "5c1cb6a5-b19b-49c9-c8a5-288af3df61e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorrt as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping nvidia-tensorrt as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorrt-cu12-libs as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorrt-cu12-bindings as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorrt==10.0.1\n",
            "  Downloading tensorrt-10.0.1.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt-cu12 (from tensorrt==10.0.1)\n",
            "  Downloading tensorrt_cu12-10.14.1.48.post1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_cu12_libs==10.14.1.48.post1 (from tensorrt-cu12->tensorrt==10.0.1)\n",
            "  Downloading tensorrt_cu12_libs-10.14.1.48.post1.tar.gz (726 bytes)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt_cu12_bindings==10.14.1.48.post1 (from tensorrt-cu12->tensorrt==10.0.1)\n",
            "  Downloading tensorrt_cu12_bindings-10.14.1.48.post1-cp312-none-manylinux_2_28_x86_64.whl.metadata (612 bytes)\n",
            "Requirement already satisfied: cuda-toolkit<13,>=12 in /usr/local/lib/python3.12/dist-packages (from cuda-toolkit[cudart]<13,>=12->tensorrt_cu12_libs==10.14.1.48.post1->tensorrt-cu12->tensorrt==10.0.1) (12.9.1)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.9.79.* (from cuda-toolkit[cudart]<13,>=12->tensorrt_cu12_libs==10.14.1.48.post1->tensorrt-cu12->tensorrt==10.0.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.9.79-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Downloading tensorrt_cu12_bindings-10.14.1.48.post1-cp312-none-manylinux_2_28_x86_64.whl (879 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m879.9/879.9 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.9.79-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m126.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tensorrt, tensorrt-cu12, tensorrt_cu12_libs\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-10.0.1-py2.py3-none-any.whl size=16331 sha256=d704500d6fa6fad797b723db7e3bf89a5e60854355e9cc84cdaab312418a87db\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/bf/33/1b957a974bacbfbc1f0642137dd8adb02c7282ede1d4c9bfe8\n",
            "  Building wheel for tensorrt-cu12 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt-cu12: filename=tensorrt_cu12-10.14.1.48.post1-py2.py3-none-any.whl size=18166 sha256=3f8b23effedce599f0933709517eb2a3710850a8c049f2b26d10db901cce0f68\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/3b/14/3f0a09edf94b2b99960813c28e648fa36fe47819d62b9a6c84\n",
            "  Building wheel for tensorrt_cu12_libs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt_cu12_libs: filename=tensorrt_cu12_libs-10.14.1.48.post1-py2.py3-none-manylinux_2_28_x86_64.whl size=3960385540 sha256=0a71baa5f774b843e3b5d1e3d7cb7912c4b5b15eff79c3c10f6728da5251fb18\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/c5/ef/0f9de5dd64bab25cabfff0cacd8958d14489f71d92d9602c4c\n",
            "Successfully built tensorrt tensorrt-cu12 tensorrt_cu12_libs\n",
            "Installing collected packages: tensorrt_cu12_bindings, nvidia-cuda-runtime-cu12, tensorrt_cu12_libs, tensorrt-cu12, tensorrt\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.9.0+cu126 requires nvidia-cuda-runtime-cu12==12.6.77; platform_system == \"Linux\", but you have nvidia-cuda-runtime-cu12 12.9.79 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cuda-runtime-cu12-12.9.79 tensorrt-10.0.1 tensorrt-cu12-10.14.1.48.post1 tensorrt_cu12_bindings-10.14.1.48.post1 tensorrt_cu12_libs-10.14.1.48.post1\n",
            "Collecting tensorrt-cu12-libs==10.0.1\n",
            "  Downloading tensorrt_cu12_libs-10.0.1.tar.gz (629 bytes)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tensorrt-cu12-bindings==10.0.1\n",
            "  Downloading tensorrt_cu12_bindings-10.0.1-cp312-none-manylinux_2_17_x86_64.whl.metadata (627 bytes)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /usr/local/lib/python3.12/dist-packages (from tensorrt-cu12-libs==10.0.1) (12.9.79)\n",
            "Downloading tensorrt_cu12_bindings-10.0.1-cp312-none-manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: tensorrt-cu12-libs\n",
            "  Building wheel for tensorrt-cu12-libs (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt-cu12-libs: filename=tensorrt_cu12_libs-10.0.1-py2.py3-none-manylinux_2_17_x86_64.whl size=1031583374 sha256=ad3b0af25d8b9f215ff877d6f565afbd084728294a1ee16c3b7f7729dcbfdff4\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/a6/98/cfec3f81f192c36e57e4f864350bb60588ca77d12d99d26d14\n",
            "Successfully built tensorrt-cu12-libs\n",
            "Installing collected packages: tensorrt-cu12-bindings, tensorrt-cu12-libs\n",
            "  Attempting uninstall: tensorrt-cu12-bindings\n",
            "    Found existing installation: tensorrt_cu12_bindings 10.14.1.48.post1\n",
            "    Uninstalling tensorrt_cu12_bindings-10.14.1.48.post1:\n",
            "      Successfully uninstalled tensorrt_cu12_bindings-10.14.1.48.post1\n",
            "  Attempting uninstall: tensorrt-cu12-libs\n",
            "    Found existing installation: tensorrt_cu12_libs 10.14.1.48.post1\n",
            "    Uninstalling tensorrt_cu12_libs-10.14.1.48.post1:\n",
            "      Successfully uninstalled tensorrt_cu12_libs-10.14.1.48.post1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorrt-cu12 10.14.1.48.post1 requires tensorrt-cu12-bindings==10.14.1.48.post1, but you have tensorrt-cu12-bindings 10.0.1 which is incompatible.\n",
            "tensorrt-cu12 10.14.1.48.post1 requires tensorrt-cu12-libs==10.14.1.48.post1, but you have tensorrt-cu12-libs 10.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorrt-cu12-bindings-10.0.1 tensorrt-cu12-libs-10.0.1\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pycuda\n",
            "  Downloading pycuda-2026.1.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting onnxruntime-tools\n",
            "  Downloading onnxruntime_tools-1.7.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting onnx-simplifier\n",
            "  Downloading onnx-simplifier-0.4.36.tar.gz (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/6a/95/9d93b8cfdd9f57abe7000cd6b9e56e2c518ce0e6bf6b312b1cf37b4e68a8/onnx-simplifier-0.4.36.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/6a/95/9d93b8cfdd9f57abe7000cd6b9e56e2c518ce0e6bf6b312b1cf37b4e68a8/onnx-simplifier-0.4.36.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.35.tar.gz (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/a5/65/56398c020ea361a879968d8f703ccf34fcb4ad172eff17b132f6948271ad/onnx-simplifier-0.4.35.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/a5/65/56398c020ea361a879968d8f703ccf34fcb4ad172eff17b132f6948271ad/onnx-simplifier-0.4.35.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.34.tar.gz (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/0a/03/9e4224cd6baf914eefd3f04512f85a67647c70b3ceb25e73c0b42262e0d0/onnx-simplifier-0.4.34.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/0a/03/9e4224cd6baf914eefd3f04512f85a67647c70b3ceb25e73c0b42262e0d0/onnx-simplifier-0.4.34.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.33.tar.gz (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/e5/cf/530c3645de1461841ad677b500726fe26b554d9f7e987f83099042fc41c8/onnx-simplifier-0.4.33.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/e5/cf/530c3645de1461841ad677b500726fe26b554d9f7e987f83099042fc41c8/onnx-simplifier-0.4.33.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.31.tar.gz (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/52/cc/1d98036bbca75422a7d22b2ea932dd548fac6275f70debaf5272f3235876/onnx-simplifier-0.4.31.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/52/cc/1d98036bbca75422a7d22b2ea932dd548fac6275f70debaf5272f3235876/onnx-simplifier-0.4.31.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.30.tar.gz (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m115.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/4f/ce/a0426de4d0a10ea601239d3c154ed37442049cd9744e0c6be3bbfa6a29bd/onnx-simplifier-0.4.30.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/4f/ce/a0426de4d0a10ea601239d3c154ed37442049cd9744e0c6be3bbfa6a29bd/onnx-simplifier-0.4.30.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.28.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/32/c2/dfdcd60db8521a132fe5d9a809385b72ab29767fc1c8a254b21b1ff9aea5/onnx-simplifier-0.4.28.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/32/c2/dfdcd60db8521a132fe5d9a809385b72ab29767fc1c8a254b21b1ff9aea5/onnx-simplifier-0.4.28.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.27.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/01/b5/1531376d145ab33e2e57a2e00123d3ace04bccfded5d677bb7ccee161f0e/onnx-simplifier-0.4.27.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/01/b5/1531376d145ab33e2e57a2e00123d3ace04bccfded5d677bb7ccee161f0e/onnx-simplifier-0.4.27.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.26.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/20/38/7228c85423d4efc13957c63e41f315bb1c4c00eea73aae6f5c1052b79a9d/onnx-simplifier-0.4.26.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/20/38/7228c85423d4efc13957c63e41f315bb1c4c00eea73aae6f5c1052b79a9d/onnx-simplifier-0.4.26.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.25.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/10/fc/e3a37891ef5ac34d64f5cdbcab0adfa8258bc4c95bd889f573051b3df169/onnx-simplifier-0.4.25.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/10/fc/e3a37891ef5ac34d64f5cdbcab0adfa8258bc4c95bd889f573051b3df169/onnx-simplifier-0.4.25.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.24.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/a4/42/899654fae8596260e151375e2b6862e01583853a0b0c135478d59d2a891f/onnx-simplifier-0.4.24.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/a4/42/899654fae8596260e151375e2b6862e01583853a0b0c135478d59d2a891f/onnx-simplifier-0.4.24.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.23.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/c4/93/28286f47c8f8d06665260d261183d58e360f867144ccfc9564c7d9127a72/onnx-simplifier-0.4.23.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/c4/93/28286f47c8f8d06665260d261183d58e360f867144ccfc9564c7d9127a72/onnx-simplifier-0.4.23.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.22.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/78/d6/e1f2ad04e8a81508b26053cc4583a4c627936ee62583b1eef483cfecba91/onnx-simplifier-0.4.22.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/78/d6/e1f2ad04e8a81508b26053cc4583a4c627936ee62583b1eef483cfecba91/onnx-simplifier-0.4.22.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.21.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/86/41/d2ab73deedb4c27d3ab73416a5a4b17611339ae59f528d67170efdcb01ec/onnx-simplifier-0.4.21.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/86/41/d2ab73deedb4c27d3ab73416a5a4b17611339ae59f528d67170efdcb01ec/onnx-simplifier-0.4.21.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.20.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/cc/10/f7c5656ecffa34894d00a87833573cf9057478c4e321a3ad701dab557bb7/onnx-simplifier-0.4.20.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/cc/10/f7c5656ecffa34894d00a87833573cf9057478c4e321a3ad701dab557bb7/onnx-simplifier-0.4.20.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.19.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/31/70/4056e2a5797dae2e3edda382e35eb5ee95ba3040e8201fe5737d9e72b1fa/onnx-simplifier-0.4.19.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/31/70/4056e2a5797dae2e3edda382e35eb5ee95ba3040e8201fe5737d9e72b1fa/onnx-simplifier-0.4.19.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.17.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m114.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/07/f2/924ec403c7030b0fd202b8755d3d368a2adce683779f05351b48698cb151/onnx-simplifier-0.4.17.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/07/f2/924ec403c7030b0fd202b8755d3d368a2adce683779f05351b48698cb151/onnx-simplifier-0.4.17.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.15.tar.gz (19.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/67/40/4853be899f646ded01044a3d88188989e9ba536a9526fbe382764252572b/onnx-simplifier-0.4.15.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/67/40/4853be899f646ded01044a3d88188989e9ba536a9526fbe382764252572b/onnx-simplifier-0.4.15.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.13.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/1e/a8/de6f65f50ac5c457f0a0fe1de8802f681b0c20650e0db23560a675319f71/onnx-simplifier-0.4.13.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.6)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/1e/a8/de6f65f50ac5c457f0a0fe1de8802f681b0c20650e0db23560a675319f71/onnx-simplifier-0.4.13.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Downloading onnx-simplifier-0.4.10.tar.gz (18.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "# 1. Uninstall any broken versions\n",
        "!pip uninstall -y tensorrt nvidia-tensorrt tensorrt-cu12-libs tensorrt-cu12-bindings\n",
        "\n",
        "# 2. Install the specific CUDA 12 packages\n",
        "!pip install tensorrt==10.0.1\n",
        "!pip install tensorrt-cu12-libs==10.0.1 tensorrt-cu12-bindings==10.0.1\n",
        "!pip install librosa tqdm jiwer pycuda onnxruntime-tools onnxruntime onnx-simplifier onnxsim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "print(\"⬇️ Downloading model.onnx (1.26 GB)...\")\n",
        "\n",
        "try:\n",
        "    # CRITICAL FIX: The file is inside the 'onnx' subfolder\n",
        "    model_path = hf_hub_download(\n",
        "        repo_id=\"onnx-community/indicwav2vec-hindi-ONNX\",\n",
        "        filename=\"onnx/model.onnx\"\n",
        "    )\n",
        "\n",
        "    # Symlink to \"model.onnx\" in current directory for easy access\n",
        "    if os.path.exists(\"model.onnx\"):\n",
        "        os.remove(\"model.onnx\")\n",
        "    os.symlink(model_path, \"model.onnx\")\n",
        "\n",
        "    print(\"✅ Download Success: model.onnx is ready.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Download Failed: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "c1a76c7e24334171ac7c2d09bf67d66b",
            "2c167d5ed4204faa80ae6b9201fac462",
            "f3f112b765d54c34af56dace949a7193",
            "021370974eb7400ca2633e063dbe9ae2",
            "76cdf49db13043eea95d61c744e2e121",
            "723bacadcb4c4e6c9fe9cbeaa3f33ecd",
            "93e692d8e1b44974bebe60685da98a12",
            "3a55707125fa4cddb06a068f0eb87acc",
            "2d3963bbd1e143a8ac154daa73b05a21",
            "8615d9d5de4045cbb25f74c42d3cbd57",
            "b9c9ade2b95b43e38f3ad18e2e4564ea"
          ]
        },
        "id": "9ZXOcQi232N8",
        "outputId": "537834d3-5b0b-43f6-b69e-ae394f374f62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⬇️ Downloading model.onnx (1.26 GB)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "onnx/model.onnx:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1a76c7e24334171ac7c2d09bf67d66b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Download Success: model.onnx is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime-tools onnxruntime onnx-simplifier onnxsim onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJ5qa6Mc50hC",
        "outputId": "666ba9b0-f495-4477-a65b-89a11b8464c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime-tools\n",
            "  Using cached onnxruntime_tools-1.7.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting onnxruntime\n",
            "  Using cached onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting onnx-simplifier\n",
            "  Using cached onnx-simplifier-0.4.36.tar.gz (21.0 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/6a/95/9d93b8cfdd9f57abe7000cd6b9e56e2c518ce0e6bf6b312b1cf37b4e68a8/onnx-simplifier-0.4.36.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/6a/95/9d93b8cfdd9f57abe7000cd6b9e56e2c518ce0e6bf6b312b1cf37b4e68a8/onnx-simplifier-0.4.36.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.35.tar.gz (20.1 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/a5/65/56398c020ea361a879968d8f703ccf34fcb4ad172eff17b132f6948271ad/onnx-simplifier-0.4.35.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/a5/65/56398c020ea361a879968d8f703ccf34fcb4ad172eff17b132f6948271ad/onnx-simplifier-0.4.35.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.34.tar.gz (20.1 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/0a/03/9e4224cd6baf914eefd3f04512f85a67647c70b3ceb25e73c0b42262e0d0/onnx-simplifier-0.4.34.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/0a/03/9e4224cd6baf914eefd3f04512f85a67647c70b3ceb25e73c0b42262e0d0/onnx-simplifier-0.4.34.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.33.tar.gz (20.1 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/e5/cf/530c3645de1461841ad677b500726fe26b554d9f7e987f83099042fc41c8/onnx-simplifier-0.4.33.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/e5/cf/530c3645de1461841ad677b500726fe26b554d9f7e987f83099042fc41c8/onnx-simplifier-0.4.33.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.31.tar.gz (20.1 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/52/cc/1d98036bbca75422a7d22b2ea932dd548fac6275f70debaf5272f3235876/onnx-simplifier-0.4.31.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/52/cc/1d98036bbca75422a7d22b2ea932dd548fac6275f70debaf5272f3235876/onnx-simplifier-0.4.31.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.30.tar.gz (20.1 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/4f/ce/a0426de4d0a10ea601239d3c154ed37442049cd9744e0c6be3bbfa6a29bd/onnx-simplifier-0.4.30.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/4f/ce/a0426de4d0a10ea601239d3c154ed37442049cd9744e0c6be3bbfa6a29bd/onnx-simplifier-0.4.30.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.28.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/32/c2/dfdcd60db8521a132fe5d9a809385b72ab29767fc1c8a254b21b1ff9aea5/onnx-simplifier-0.4.28.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/32/c2/dfdcd60db8521a132fe5d9a809385b72ab29767fc1c8a254b21b1ff9aea5/onnx-simplifier-0.4.28.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.27.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/01/b5/1531376d145ab33e2e57a2e00123d3ace04bccfded5d677bb7ccee161f0e/onnx-simplifier-0.4.27.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/01/b5/1531376d145ab33e2e57a2e00123d3ace04bccfded5d677bb7ccee161f0e/onnx-simplifier-0.4.27.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.26.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/20/38/7228c85423d4efc13957c63e41f315bb1c4c00eea73aae6f5c1052b79a9d/onnx-simplifier-0.4.26.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/20/38/7228c85423d4efc13957c63e41f315bb1c4c00eea73aae6f5c1052b79a9d/onnx-simplifier-0.4.26.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.25.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/10/fc/e3a37891ef5ac34d64f5cdbcab0adfa8258bc4c95bd889f573051b3df169/onnx-simplifier-0.4.25.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/10/fc/e3a37891ef5ac34d64f5cdbcab0adfa8258bc4c95bd889f573051b3df169/onnx-simplifier-0.4.25.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.24.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/a4/42/899654fae8596260e151375e2b6862e01583853a0b0c135478d59d2a891f/onnx-simplifier-0.4.24.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/a4/42/899654fae8596260e151375e2b6862e01583853a0b0c135478d59d2a891f/onnx-simplifier-0.4.24.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.23.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/c4/93/28286f47c8f8d06665260d261183d58e360f867144ccfc9564c7d9127a72/onnx-simplifier-0.4.23.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/c4/93/28286f47c8f8d06665260d261183d58e360f867144ccfc9564c7d9127a72/onnx-simplifier-0.4.23.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.22.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/78/d6/e1f2ad04e8a81508b26053cc4583a4c627936ee62583b1eef483cfecba91/onnx-simplifier-0.4.22.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/78/d6/e1f2ad04e8a81508b26053cc4583a4c627936ee62583b1eef483cfecba91/onnx-simplifier-0.4.22.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.21.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/86/41/d2ab73deedb4c27d3ab73416a5a4b17611339ae59f528d67170efdcb01ec/onnx-simplifier-0.4.21.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/86/41/d2ab73deedb4c27d3ab73416a5a4b17611339ae59f528d67170efdcb01ec/onnx-simplifier-0.4.21.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.20.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/cc/10/f7c5656ecffa34894d00a87833573cf9057478c4e321a3ad701dab557bb7/onnx-simplifier-0.4.20.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/cc/10/f7c5656ecffa34894d00a87833573cf9057478c4e321a3ad701dab557bb7/onnx-simplifier-0.4.20.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.19.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/31/70/4056e2a5797dae2e3edda382e35eb5ee95ba3040e8201fe5737d9e72b1fa/onnx-simplifier-0.4.19.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/31/70/4056e2a5797dae2e3edda382e35eb5ee95ba3040e8201fe5737d9e72b1fa/onnx-simplifier-0.4.19.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.17.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/07/f2/924ec403c7030b0fd202b8755d3d368a2adce683779f05351b48698cb151/onnx-simplifier-0.4.17.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/07/f2/924ec403c7030b0fd202b8755d3d368a2adce683779f05351b48698cb151/onnx-simplifier-0.4.17.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.15.tar.gz (19.2 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/67/40/4853be899f646ded01044a3d88188989e9ba536a9526fbe382764252572b/onnx-simplifier-0.4.15.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/67/40/4853be899f646ded01044a3d88188989e9ba536a9526fbe382764252572b/onnx-simplifier-0.4.15.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.13.tar.gz (18.1 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package onnx-simplifier produced metadata for project name onnxsim. Fix your #egg=onnx-simplifier fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/1e/a8/de6f65f50ac5c457f0a0fe1de8802f681b0c20650e0db23560a675319f71/onnx-simplifier-0.4.13.tar.gz (from https://pypi.org/simple/onnx-simplifier/) (requires-python:>=3.6)\u001b[0m: \u001b[33mRequested onnxsim from https://files.pythonhosted.org/packages/1e/a8/de6f65f50ac5c457f0a0fe1de8802f681b0c20650e0db23560a675319f71/onnx-simplifier-0.4.13.tar.gz has inconsistent name: expected 'onnx-simplifier', but metadata has 'onnxsim'\u001b[0m\n",
            "  Using cached onnx-simplifier-0.4.10.tar.gz (18.1 MB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BvuKclO-6mNx",
        "outputId": "84500c28-2bf0-4802-ff83-1f2846fb8543"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.20.1-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
            "Downloading onnx-1.20.1-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxsim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flmLjAX86u1h",
        "outputId": "624325d7-6839-47d9-fecd-9a211cb080f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxsim\n",
            "  Downloading onnxsim-0.4.36.tar.gz (21.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/21.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/21.0 MB\u001b[0m \u001b[31m249.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/21.0 MB\u001b[0m \u001b[31m192.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m210.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m210.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m110.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (from onnxsim) (1.20.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from onnxsim) (13.9.4)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.12/dist-packages (from onnx->onnxsim) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx->onnxsim) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx->onnxsim) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx->onnxsim) (0.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->onnxsim) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->onnxsim) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n",
            "Building wheels for collected packages: onnxsim\n",
            "  Building wheel for onnxsim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for onnxsim: filename=onnxsim-0.4.36-cp312-cp312-linux_x86_64.whl size=2200380 sha256=51aec21bb55afc182d3ba17d59e1119a80067ab62d98cf7533a45648c933e8c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/5d/cc/db1350d9fabfe7f8442b5d97aff2ff543fc253277f71a6508f\n",
            "Successfully built onnxsim\n",
            "Installing collected packages: onnxsim\n",
            "Successfully installed onnxsim-0.4.36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnxsim import simplify\n",
        "\n",
        "def optimize_graph():\n",
        "    input_path = \"model.onnx\"\n",
        "    output_path = \"model_optimized.onnx\"\n",
        "\n",
        "    print(f\"Loading {input_path}...\")\n",
        "    model = onnx.load(input_path)\n",
        "\n",
        "    # 2. Simplify\n",
        "    # This performs constant folding, removes unused nodes, and fixes the 'toposort' error\n",
        "    print(\"Optimizing and Simplifying graph...\")\n",
        "    model_simp, check = simplify(model)\n",
        "\n",
        "    assert check, \"❌ Simplified ONNX model could not be validated!\"\n",
        "\n",
        "    # 3. Save\n",
        "    onnx.save(model_simp, output_path)\n",
        "    print(f\"✅ Success! Optimized Graph saved to: {output_path}\")\n",
        "    print(\"This graph is now strictly compliant for TensorRT.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    optimize_graph()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "QivWspOlrco3",
        "outputId": "175e9234-0aeb-408d-c386-74ed1f62f204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1;35mInstalling onnxruntime by `/usr/bin/python3 -m pip install onnxruntime`, please wait for a moment..\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Installing onnxruntime by `/usr/bin/python3 -m pip install onnxruntime`, please wait for a moment..</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model.onnx...\n",
            "Optimizing and Simplifying graph...\n",
            "✅ Success! Optimized Graph saved to: model_optimized.onnx\n",
            "This graph is now strictly compliant for TensorRT.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import os\n",
        "import tensorrt as trt\n",
        "\n",
        "# Ensure NVIDIA libraries are visible\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/lib64-nvidia'\n",
        "\n",
        "def build_engine():\n",
        "    logger = trt.Logger(trt.Logger.VERBOSE)\n",
        "    builder = trt.Builder(logger)\n",
        "\n",
        "    # 1. Network & Parser\n",
        "    flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "    network = builder.create_network(flag)\n",
        "    config = builder.create_builder_config()\n",
        "    parser = trt.OnnxParser(network, logger)\n",
        "\n",
        "    # 2. Parse the SIMPLIFIED ONNX\n",
        "    onnx_path = \"model_optimized.onnx\"\n",
        "    print(f\"Parsing Optimized ONNX: {onnx_path}\")\n",
        "\n",
        "    with open(onnx_path, 'rb') as model:\n",
        "        if not parser.parse(model.read()):\n",
        "            print(\"❌ Parser Failed. Specific Errors:\")\n",
        "            for error in range(parser.num_errors):\n",
        "                print(parser.get_error(error))\n",
        "            return\n",
        "\n",
        "    # 3. Dynamic Shapes (Critical for Wav2Vec2)\n",
        "    profile = builder.create_optimization_profile()\n",
        "    input_name = network.get_input(0).name\n",
        "\n",
        "    # Min: 1 sec, Opt: 5 sec, Max: 10 sec\n",
        "    profile.set_shape(input_name, (1, 16000), (1, 80000), (4, 160000))\n",
        "    config.add_optimization_profile(profile)\n",
        "\n",
        "    # 4. FP16 Optimization\n",
        "    # This enables Tensor Cores (Hardware Acceleration)\n",
        "    if builder.platform_has_fast_fp16:\n",
        "        config.set_flag(trt.BuilderFlag.FP16)\n",
        "        print(\"🚀 FP16 Layer Fusion Enabled.\")\n",
        "\n",
        "    # 5. Build\n",
        "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 * 1024 * 1024 * 1024)\n",
        "    print(\"Building TensorRT Engine... (Wait ~5-10 mins)\")\n",
        "\n",
        "    plan = builder.build_serialized_network(network, config)\n",
        "\n",
        "    if plan is None:\n",
        "        print(\"❌ Build Failed. Check logs above.\")\n",
        "        return\n",
        "\n",
        "    with open(\"model.engine\", \"wb\") as f:\n",
        "        f.write(plan)\n",
        "\n",
        "    print(\"✅ Success! Engine saved as 'model.engine'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhVoeofJv73q",
        "outputId": "288f618b-835a-40e0-b638-8d7390de13c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing Optimized ONNX: model_optimized.onnx\n",
            "🚀 FP16 Layer Fusion Enabled.\n",
            "Building TensorRT Engine... (Wait ~5-10 mins)\n",
            "✅ Success! Engine saved as 'model.engine'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "# This will prompt you to enter your token from huggingface.co/settings/tokens\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "8c5d0dbee59e4b16a93a12efda3b93da",
            "bea2f9c1ba324322b2160f3be69ce5d2",
            "d76fcd4b14df4ca7ba2e2a8032a59693",
            "52b31945523245e7a303f5585c247fd8",
            "746439369cb147158b46bbd3127db2cb",
            "0f9ed689ca3e4dc5871161267cbd06a1",
            "3c71c16389fe4525a8b6ab7c51369f93",
            "2f6113e9f2a342e789db7b9c6bd230a9",
            "c772049bff394449b8ec704befe829c7",
            "4cd089bdaf5e428d819ac5362ad44109",
            "73b11b4febc342e09a05e304404fed1f",
            "9165c3c74e264c78817a1d1f59ac6bbb",
            "decf86e6afbb4808b9c98b1a335fcc0c",
            "d2a4b414d5324e2b99cde78adee24de4",
            "cf620670c4e4473e99d732e66d4de4f0",
            "6f1e0d7a9bd548418b5a4741d3ae6f90",
            "2ea6c0db0d0e4de8ba3610504afc1142",
            "037d19baa8e64708b9d07f53aadd6e96",
            "ef8fb347c8e049a0918e6793704c6ccb",
            "e53eae35687d4b4c94875a04f98a2dde"
          ]
        },
        "id": "GsgK7vQf1VT_",
        "outputId": "34409872-dc8c-4dcb-db5e-bad756ad2e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c5d0dbee59e4b16a93a12efda3b93da"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNBTKY7D-LQz",
        "outputId": "afa3cd7f-496f-45a7-bd83-db80fd26b911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Using cached pycuda-2026.1.tar.gz (1.7 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.2.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from pycuda) (4.5.1)\n",
            "Requirement already satisfied: mako in /usr/local/lib/python3.12/dist-packages (from pycuda) (1.3.10)\n",
            "Collecting siphash24>=1.6 (from pytools>=2011.2->pycuda)\n",
            "  Downloading siphash24-1.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from pytools>=2011.2->pycuda) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from mako->pycuda) (3.0.3)\n",
            "Downloading pytools-2025.2.5-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.8/98.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading siphash24-1.8-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2026.1-cp312-cp312-linux_x86_64.whl size=659449 sha256=d334c79160d534ed825c5c9bdce52ca94195fcc36571d3f4356ccbcb4667165f\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/2a/71/75ec0cc316cc0ff494bfffa2935e02580129cb7f859a0cfd8f\n",
            "Successfully built pycuda\n",
            "Installing collected packages: siphash24, pytools, pycuda\n",
            "Successfully installed pycuda-2026.1 pytools-2025.2.5 siphash24-1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa tqdm jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agk2Ch69_DDo",
        "outputId": "bf18dc39-a174-4a2b-da22-995202036dd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Collecting jiwer\n",
            "  Using cached jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
            "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2026.1.4)\n",
            "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
            "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from datasets import load_dataset\n",
        "from jiwer import wer\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import io\n",
        "\n",
        "def run_gpu_benchmark():\n",
        "    engine_path = \"model.engine\"\n",
        "    model_id = \"ai4bharat/indicwav2vec-hindi\"\n",
        "\n",
        "    # 1. Setup TensorRT Runtime & Context\n",
        "    logger = trt.Logger(trt.Logger.INFO)\n",
        "    runtime = trt.Runtime(logger)\n",
        "    with open(engine_path, \"rb\") as f:\n",
        "        engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    context = engine.create_execution_context()\n",
        "\n",
        "    # 2. Setup Original Model for Baseline\n",
        "    # This uses your logged-in token automatically\n",
        "    print(\"Loading Original Model for Baseline...\")\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "    pt_model = Wav2Vec2ForCTC.from_pretrained(model_id).to(\"cuda\")\n",
        "    pt_model.eval()\n",
        "\n",
        "    # 3. Load Evaluation Data\n",
        "    print(\"Loading Hindi test samples...\")\n",
        "    # Using 'streaming=True' to avoid downloading the whole dataset at once\n",
        "    dataset = load_dataset(\"MatrixSpeechAI/Common_voice_hindi_denoised\", split=\"train\", streaming=True).decode(False)\n",
        "\n",
        "    trt_latencies, pt_latencies = [], []\n",
        "    trt_preds, pt_preds, truths = [], [], []\n",
        "\n",
        "    print(\"Benchmarking 50 samples...\")\n",
        "    for i, item in enumerate(tqdm(dataset.take(50))):\n",
        "        # Prepare Audio\n",
        "        audio, _ = librosa.load(io.BytesIO(item[\"audio\"][\"bytes\"]), sr=16000)\n",
        "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        input_values = inputs.input_values.to(\"cuda\")\n",
        "        input_np = input_values.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # --- PYTORCH BASELINE ---\n",
        "        torch.cuda.synchronize() # Ensure GPU is ready\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            pt_logits = pt_model(input_values).logits\n",
        "        torch.cuda.synchronize()\n",
        "        pt_latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "        pt_pred_ids = torch.argmax(pt_logits, dim=-1)\n",
        "        pt_preds.append(processor.batch_decode(pt_pred_ids)[0])\n",
        "\n",
        "        # --- TENSORRT INFERENCE ---\n",
        "        # 1. Set input shape for dynamic engine\n",
        "        context.set_input_shape(\"input_values\", input_np.shape)\n",
        "\n",
        "        # 2. Allocate memory on GPU\n",
        "        d_input = cuda.mem_alloc(input_np.nbytes)\n",
        "\n",
        "        # Wav2Vec2 typically reduces sequence length by 320x in the feature extractor\n",
        "        output_shape = (input_np.shape[0], input_np.shape[1] // 320, 108)\n",
        "        output_np = np.empty(output_shape, dtype=np.float32)\n",
        "        d_output = cuda.mem_alloc(output_np.nbytes)\n",
        "\n",
        "        # 3. Transfer, Execute, and Retrieve\n",
        "        cuda.memcpy_htod(d_input, input_np)\n",
        "\n",
        "        start = time.time()\n",
        "        context.execute_v2([int(d_input), int(d_output)])\n",
        "        cuda.memcpy_dtoh(output_np, d_output)\n",
        "        trt_latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "        trt_pred_ids = np.argmax(output_np, axis=-1)\n",
        "        trt_preds.append(processor.batch_decode(trt_pred_ids)[0])\n",
        "        truths.append(item[\"transcription\"])\n",
        "\n",
        "    # 4. REPORT\n",
        "    avg_pt = np.mean(pt_latencies)\n",
        "    avg_trt = np.mean(trt_latencies)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{'METRIC':<25} | {'PYTORCH (GPU)':<15} | {'TENSORRT (FP16)':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Avg Latency (ms)':<25} | {avg_pt:<15.2f} | {avg_trt:<15.2f}\")\n",
        "    print(f\"{'Word Error Rate (%)':<25} | {wer(truths, pt_preds)*100:<15.2f} | {wer(truths, trt_preds)*100:<15.2f}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"🚀 TOTAL GPU SPEEDUP: {avg_pt/avg_trt:.2f}x faster\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_gpu_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 950,
          "referenced_widgets": [
            "1bea67f326134ae699c168439181dffb",
            "c5f4c435a98f4e81b31ae629a65ac36a",
            "3ae60ea516dd4092acd685ede7b4ab1e",
            "62ad0d6ccd794e6da0c474bf3e9f637f",
            "9f93e80c525f4e279815e6e9fb9641e4",
            "a1d4dd78af1140c6b52f708f17e6cf53",
            "3cc042f62f19478fb40a42cee72d87be",
            "7b264925afcf4f56aa823b27fc35d3a1",
            "9150a51f7e5a49c48ffe2dbf9b571450",
            "01bfc21a5ef642e78edcb9363e3f13d1",
            "9252ec5c6cf5412e9a4fe714ef488278",
            "71c95d777d1d4fff8b17bd1011452945",
            "95349f6af5b245988d37fc43aa7a5e51",
            "1fcc786967f04607b3b9d0cd1befa96f",
            "1509b7c452b645baa4fbf66dfabd50cd",
            "4d3902c5bd8548198f6bd21c3934c6cc",
            "a4184ce3ff5f4d1996604158b571ca6f",
            "485519f1c00743db933590fcb19ca8fb",
            "b47d4403095f4999a0f5f4aee1df5897",
            "9665e115e1dd470c85b7c54ab727305f",
            "30c688cbbded4187bb9532fc936c7efe",
            "29e5307166c4445f8b76c20e53ac98b9",
            "66b36b8afdd84c8793a0064166ac748f",
            "1776b82a20094df098816ab39197a5da",
            "ae0b2bdd5a96432a86eaec11ba134671",
            "06dd1168f30d47e78eceb8e689f0d4be",
            "8bc5356225d743589258581cb6e8b85a",
            "572b990f2bb84291bbbee010108c02f3",
            "798c9f743dc145e9b7c7d027834b51d5",
            "65f6f3588fe24893a1eb7cc40f16f10a",
            "120a67efdb584492bbd47b5f337eeb1c",
            "828a635f89a9451c8c358b65168210c5",
            "f82c7763ab2b49d0889ccb0446afeb82",
            "a0007ce5c346483ea0bed61643a0be33",
            "1346ac69b4034a93bcd6f5d9a73f0221",
            "8f10180623a94ce8b5f5da5b253103f0",
            "b61c4f5a32cc4022bfaca04436d21034",
            "5c9e14c332c649d7880ce921814b0f23",
            "5d0db13c177c44fc8531b7ab651fb220",
            "6e077998a8a14773be1594ee515c234c",
            "a97e01a020604bf988389a9353fc9fcf",
            "f6cfabd373bd4b00a13a67c3fa2161ad",
            "40d939681e39470da93fd56941f09345",
            "a5c561e3a5d140e98fdd8c2891779ce3",
            "82adc37fed4d4df9abaf74546a92f296",
            "235c09f91368425dbbd9e681771af649",
            "c6f90ad002134686a2a5c1e174b0e3e2",
            "0fa0a66489414370bb9ae2cece4b6892",
            "3994bfb7f52e4bdc85b036374fdc49f1",
            "88a5f94919f54bcea2f6f0a404a04999",
            "d553b5ae4b4c44469d49ba0740acb4bd",
            "39f396ead69b4ce08d55c4d1d4a3e1f9",
            "bdd0a0d9142747c4b606a87e9143f1a3",
            "ef26f1508f4c4df8a08b3088dc12d738",
            "d297b8c8270b44baa5f3045b8cf16c48",
            "45d42cd3bea1442180e4f9501e26a2b9",
            "d132c36ee34b47508ae9c0c3bc9cd8e8",
            "4b4f8a74dc9d489eb3b49e973179c57f",
            "ec45338ed9fb49c48406ae0fc2f0ee2a",
            "7a9fcea100814249ade7360eb944e2d7",
            "2413b7e865874adb98e4fc8e40578d26",
            "bb9121e647314a3f85f095e7fd7814df",
            "8c61a4023a964b7983360a2a29f076fe",
            "8a3c0e0c14dc468ea3857f9705917b2c",
            "90626c86f9a94ca49a563450067b4da3",
            "00d979a90a954dc2ac8e3a88c019d0f0",
            "b7cdc22d026a46c59de5ceeda7396333",
            "9b63a6ee23d046ea9500462ceaff4cf7",
            "57c923eefe9b495cac534ba3ed6d4e8b",
            "d6de7a9ebf5840fea773b59954f3cd8a",
            "0eb7c6608f8d468c9bd7cf938e3e165c",
            "db4d30cad366463596262252de9d1c90",
            "d4e89f93fff34034b3f2d883da44892b",
            "121c77289981414faf36a6cd69b4bef2",
            "83f71ee1296b4ba186ab966a065e4929",
            "d4ed62f7131a41a190088e3c4524bc01",
            "8e8690141b5d48968830a5e26118c651",
            "c0738358416549fa88d1efcd3d112200",
            "1beff33c3a3f4d0499c0fd11150b15f8",
            "12133294e0f2461bbdaa5a049ccc8ee8",
            "390276c5bd9d43a7ac3af3d8dfbb855c",
            "486f5651dbb9444597919ad2927c69b2",
            "4ae1aaf9daa243c381e0bb5cf8a247be",
            "0534285e54c446518b267b19ad3da176",
            "402ee573140142a6bbac8feb15422e7a",
            "946fa54f54a24f9789e6d7770797e39a",
            "606c9266a180450daf47e4c38f352e7b",
            "470f887c2b404d05b079c71b9fbe4e6e"
          ]
        },
        "id": "pvDUBGA825b1",
        "outputId": "4e07df87-14ed-4a79-f442-b033ac79cc9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Original Model for Baseline...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bea67f326134ae699c168439181dffb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/257 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71c95d777d1d4fff8b17bd1011452945"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/741 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66b36b8afdd84c8793a0064166ac748f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0007ce5c346483ea0bed61643a0be33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82adc37fed4d4df9abaf74546a92f296"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45d42cd3bea1442180e4f9501e26a2b9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7cdc22d026a46c59de5ceeda7396333"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Hindi test samples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/436 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0738358416549fa88d1efcd3d112200"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmarking 50 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:30, 30.59s/it]\u001b[A\n",
            "2it [00:30, 12.66s/it]\u001b[A\n",
            "3it [00:30,  6.93s/it]\u001b[A\n",
            "5it [00:30,  3.06s/it]\u001b[A\n",
            "7it [00:31,  1.73s/it]\u001b[A\n",
            "9it [00:31,  1.09s/it]\u001b[A\n",
            "11it [00:31,  1.37it/s]\u001b[A\n",
            "13it [00:31,  1.97it/s]\u001b[A\n",
            "15it [00:31,  2.75it/s]\u001b[A\n",
            "17it [00:31,  3.52it/s]\u001b[A\n",
            "19it [00:32,  4.53it/s]\u001b[A\n",
            "21it [00:32,  5.26it/s]\u001b[A\n",
            "23it [00:32,  6.14it/s]\u001b[A\n",
            "25it [00:32,  7.28it/s]\u001b[A\n",
            "27it [00:32,  8.29it/s]\u001b[A\n",
            "29it [00:33,  7.90it/s]\u001b[A\n",
            "31it [00:33,  8.19it/s]\u001b[A\n",
            "33it [00:33,  9.53it/s]\u001b[A\n",
            "35it [00:33,  9.79it/s]\u001b[A\n",
            "37it [00:33, 10.43it/s]\u001b[A\n",
            "39it [00:34, 10.89it/s]\u001b[A\n",
            "41it [00:34, 11.12it/s]\u001b[A\n",
            "43it [00:34, 11.18it/s]\u001b[A\n",
            "45it [00:34, 10.24it/s]\u001b[A\n",
            "47it [00:34, 10.51it/s]\u001b[A\n",
            "50it [00:35,  1.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "METRIC                    | PYTORCH (GPU)   | TENSORRT (FP16)\n",
            "------------------------------------------------------------\n",
            "Avg Latency (ms)          | 82.52           | 17.95          \n",
            "Word Error Rate (%)       | 35.39           | 159.55         \n",
            "============================================================\n",
            "🚀 TOTAL GPU SPEEDUP: 4.60x faster\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "from datasets import load_dataset\n",
        "import librosa\n",
        "import io\n",
        "\n",
        "def ctc_decode_debug(logits, processor):\n",
        "    # 1. Get ID with highest probability\n",
        "    pred_ids = np.argmax(logits, axis=-1)[0]\n",
        "\n",
        "    # 2. Decode RAW (What does the model actually see before cleaning?)\n",
        "    # This helps us see if it's \"mmmmmaaaaa\" or random garbage\n",
        "    raw_text = processor.decode(pred_ids)\n",
        "\n",
        "    # 3. Clean Decode (Group repeats)\n",
        "    grouped_ids = [x for i, x in enumerate(pred_ids) if i == 0 or x != pred_ids[i-1]]\n",
        "    blank_id = processor.tokenizer.pad_token_id\n",
        "    cleaned_ids = [x for x in grouped_ids if x != blank_id]\n",
        "    cleaned_text = processor.decode(cleaned_ids)\n",
        "\n",
        "    return raw_text, cleaned_text\n",
        "\n",
        "def run_debug():\n",
        "    engine_path = \"model.engine\"\n",
        "    model_id = \"ai4bharat/indicwav2vec-hindi\"\n",
        "\n",
        "    # Load Models\n",
        "    print(\"Loading models...\")\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "\n",
        "    # PyTorch Setup\n",
        "    pt_model = Wav2Vec2ForCTC.from_pretrained(model_id).to(\"cuda\")\n",
        "    pt_model.eval()\n",
        "\n",
        "    # TensorRT Setup\n",
        "    logger = trt.Logger(trt.Logger.ERROR)\n",
        "    with open(engine_path, \"rb\") as f, trt.Runtime(logger) as runtime:\n",
        "        engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    context = engine.create_execution_context()\n",
        "\n",
        "    # Load Data\n",
        "    print(\"Loading 5 samples...\")\n",
        "    dataset = load_dataset(\"MatrixSpeechAI/Common_voice_hindi_denoised\", split=\"train\", streaming=True).decode(False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"{'ROLE':<10} | {'TEXT OUTPUT'}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for i, item in enumerate(dataset.take(5)):\n",
        "        audio, _ = librosa.load(io.BytesIO(item[\"audio\"][\"bytes\"]), sr=16000)\n",
        "\n",
        "        # --- PYTORCH ---\n",
        "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        pt_logits = pt_model(inputs.input_values.to(\"cuda\")).logits\n",
        "        pt_pred_ids = torch.argmax(pt_logits, dim=-1)\n",
        "        pt_text = processor.batch_decode(pt_pred_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        # --- TENSORRT ---\n",
        "        input_np = inputs.input_values.numpy().astype(np.float32)\n",
        "        context.set_input_shape(\"input_values\", input_np.shape)\n",
        "        d_input = cuda.mem_alloc(input_np.nbytes)\n",
        "\n",
        "        seq_len = input_np.shape[1] // 320\n",
        "        output_shape = (1, seq_len, 108)\n",
        "        output_np = np.empty(output_shape, dtype=np.float32)\n",
        "        d_output = cuda.mem_alloc(output_np.nbytes)\n",
        "\n",
        "        cuda.memcpy_htod(d_input, input_np)\n",
        "        context.execute_v2([int(d_input), int(d_output)])\n",
        "        cuda.memcpy_dtoh(output_np, d_output)\n",
        "\n",
        "        # Get both Raw (stuttering) and Cleaned text\n",
        "        raw_trt, clean_trt = ctc_decode_debug(output_np, processor)\n",
        "\n",
        "        # --- PRINT COMPARISON ---\n",
        "        print(f\"SAMPLE {i+1}:\")\n",
        "        print(f\"Truth     : {item['transcription']}\")\n",
        "        print(f\"PyTorch   : {pt_text}\")\n",
        "        print(f\"TRT (Raw) : {raw_trt[:100]}...\") # Show first 100 chars of raw to check for stutter\n",
        "        print(f\"TRT (Fix) : {clean_trt}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_debug()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJLBZwlY3Bcm",
        "outputId": "bba99076-83d2-4d91-9769-32b85db8e8ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading models...\n",
            "Loading 5 samples...\n",
            "\n",
            "================================================================================\n",
            "ROLE       | TEXT OUTPUT\n",
            "================================================================================\n",
            "SAMPLE 1:\n",
            "Truth     : तुम्हारे पास तीन महीने बचे हैं।\n",
            "PyTorch   : तुम्हारे पास तीन महीने बच्चे हैं\n",
            "TRT (Raw) : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम...\n",
            "TRT (Fix) : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउतचआत<unk>ऋ</s>ेह<unk></s><unk>औ<unk>्<unk>शञऐ<unk>चहड<unk>जइ<unk>ृ<unk>रग<unk>ॉफ<unk>्ॉेॅूुीि़हश<unk>ँ<unk>ऋभब<unk>\n",
            "--------------------------------------------------------------------------------\n",
            "SAMPLE 2:\n",
            "Truth     : मैं खाती हूँ।\n",
            "PyTorch   : मैं कॉती हू\n",
            "TRT (Raw) : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम <unk>द<unk>श...\n",
            "TRT (Fix) : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम <unk>द<unk>श\n",
            "--------------------------------------------------------------------------------\n",
            "SAMPLE 3:\n",
            "Truth     : जब मैं छोटा था मैं हमेशा जल्दी सोकर उठा करता था।\n",
            "PyTorch   : जब मैं छोटा था मैं हमेशा जल्दी सोकर उठता था\n",
            "TRT (Raw) : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम...\n",
            "TRT (Fix) : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम टय<unk>ऑ<unk>ौङख<unk>क ञ<unk>फतहड<unk> <unk>ष<unk>ञफ<unk>औम<unk>ढ़<unk>ः<unk>कधि<unk>ओॉ<unk>फ<unk>ञगऋढछलदो<unk>छभ<unk>षउज<unk>खौ<unk>अॉभ<unk>क\n",
            "--------------------------------------------------------------------------------\n",
            "SAMPLE 4:\n",
            "Truth     : बाईं ओर को एक गुप्त रास्ता है।\n",
            "PyTorch   : बाई और को एक मुक्त रास्ता है\n",
            "TRT (Raw) : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम...\n",
            "TRT (Fix) : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउप<unk>द<unk>शु्<unk>ू<s><unk>सझगउ</s><unk>ऐइ<unk>डूॅखः<unk>ो<unk>म<unk>बठै<unk>िोध\n",
            "--------------------------------------------------------------------------------\n",
            "SAMPLE 5:\n",
            "Truth     : मुझे इस लड़की से नफ़रत है।\n",
            "PyTorch   : मुझे इस लड़की से नफरत है िस्तार ने द\n",
            "TRT (Raw) : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम...\n",
            "TRT (Fix) : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथ<unk>हउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथो\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorrt as trt\n",
        "\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/lib64-nvidia'\n",
        "\n",
        "def build_stable_fp32_engine():\n",
        "    logger = trt.Logger(trt.Logger.VERBOSE)\n",
        "    builder = trt.Builder(logger)\n",
        "\n",
        "    flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "    network = builder.create_network(flag)\n",
        "    config = builder.create_builder_config()\n",
        "    parser = trt.OnnxParser(network, logger)\n",
        "\n",
        "    # USE THE SIMPLIFIED ONNX (It's cleaner)\n",
        "    onnx_path = \"model_optimized.onnx\"\n",
        "    print(f\"Parsing: {onnx_path}\")\n",
        "\n",
        "    with open(onnx_path, 'rb') as model:\n",
        "        if not parser.parse(model.read()):\n",
        "            print(\"❌ Parser Failed\")\n",
        "            return\n",
        "\n",
        "    # Dynamic Shapes\n",
        "    profile = builder.create_optimization_profile()\n",
        "    input_name = network.get_input(0).name\n",
        "    profile.set_shape(input_name, (1, 16000), (1, 80000), (4, 160000))\n",
        "    config.add_optimization_profile(profile)\n",
        "\n",
        "    # --- CRITICAL CHANGE: DISABLE FP16 ---\n",
        "    # We comment out the FP16 flag to ensure mathematical stability\n",
        "    # config.set_flag(trt.BuilderFlag.FP16)\n",
        "    print(\"ℹ️ Building in FP32 Mode for Stability...\")\n",
        "\n",
        "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 * 1024 * 1024 * 1024)\n",
        "\n",
        "    plan = builder.build_serialized_network(network, config)\n",
        "    if plan:\n",
        "        with open(\"model.engine\", \"wb\") as f:\n",
        "            f.write(plan)\n",
        "        print(\"✅ Success! Stable FP32 Engine created.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_stable_fp32_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIwlOVSy4I1P",
        "outputId": "12bca3a3-961e-4b56-a972-3d001c829356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing: model_optimized.onnx\n",
            "ℹ️ Building in FP32 Mode for Stability...\n",
            "✅ Success! Stable FP32 Engine created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from datasets import load_dataset\n",
        "from jiwer import wer\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import io\n",
        "\n",
        "def run_gpu_benchmark():\n",
        "    engine_path = \"model.engine\"\n",
        "    model_id = \"ai4bharat/indicwav2vec-hindi\"\n",
        "\n",
        "    # 1. Setup TensorRT Runtime & Context\n",
        "    logger = trt.Logger(trt.Logger.INFO)\n",
        "    runtime = trt.Runtime(logger)\n",
        "    with open(engine_path, \"rb\") as f:\n",
        "        engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    context = engine.create_execution_context()\n",
        "\n",
        "    # 2. Setup Original Model for Baseline\n",
        "    # This uses your logged-in token automatically\n",
        "    print(\"Loading Original Model for Baseline...\")\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "    pt_model = Wav2Vec2ForCTC.from_pretrained(model_id).to(\"cuda\")\n",
        "    pt_model.eval()\n",
        "\n",
        "    # 3. Load Evaluation Data\n",
        "    print(\"Loading Hindi test samples...\")\n",
        "    # Using 'streaming=True' to avoid downloading the whole dataset at once\n",
        "    dataset = load_dataset(\"MatrixSpeechAI/Common_voice_hindi_denoised\", split=\"train\", streaming=True).decode(False)\n",
        "\n",
        "    trt_latencies, pt_latencies = [], []\n",
        "    trt_preds, pt_preds, truths = [], [], []\n",
        "\n",
        "    print(\"Benchmarking 50 samples...\")\n",
        "    for i, item in enumerate(tqdm(dataset.take(50))):\n",
        "        # Prepare Audio\n",
        "        audio, _ = librosa.load(io.BytesIO(item[\"audio\"][\"bytes\"]), sr=16000)\n",
        "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        input_values = inputs.input_values.to(\"cuda\")\n",
        "        input_np = input_values.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # --- PYTORCH BASELINE ---\n",
        "        torch.cuda.synchronize() # Ensure GPU is ready\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            pt_logits = pt_model(input_values).logits\n",
        "        torch.cuda.synchronize()\n",
        "        pt_latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "        pt_pred_ids = torch.argmax(pt_logits, dim=-1)\n",
        "        pt_preds.append(processor.batch_decode(pt_pred_ids)[0])\n",
        "\n",
        "        # --- TENSORRT INFERENCE ---\n",
        "        # 1. Set input shape for dynamic engine\n",
        "        context.set_input_shape(\"input_values\", input_np.shape)\n",
        "\n",
        "        # 2. Allocate memory on GPU\n",
        "        d_input = cuda.mem_alloc(input_np.nbytes)\n",
        "\n",
        "        # Wav2Vec2 typically reduces sequence length by 320x in the feature extractor\n",
        "        output_shape = (input_np.shape[0], input_np.shape[1] // 320, 108)\n",
        "        output_np = np.empty(output_shape, dtype=np.float32)\n",
        "        d_output = cuda.mem_alloc(output_np.nbytes)\n",
        "\n",
        "        # 3. Transfer, Execute, and Retrieve\n",
        "        cuda.memcpy_htod(d_input, input_np)\n",
        "\n",
        "        start = time.time()\n",
        "        context.execute_v2([int(d_input), int(d_output)])\n",
        "        cuda.memcpy_dtoh(output_np, d_output)\n",
        "        trt_latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "        trt_pred_ids = np.argmax(output_np, axis=-1)\n",
        "        trt_preds.append(processor.batch_decode(trt_pred_ids)[0])\n",
        "        truths.append(item[\"transcription\"])\n",
        "\n",
        "    # 4. REPORT\n",
        "    avg_pt = np.mean(pt_latencies)\n",
        "    avg_trt = np.mean(trt_latencies)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{'METRIC':<25} | {'PYTORCH (GPU)':<15} | {'TENSORRT (FP16)':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Avg Latency (ms)':<25} | {avg_pt:<15.2f} | {avg_trt:<15.2f}\")\n",
        "    print(f\"{'Word Error Rate (%)':<25} | {wer(truths, pt_preds)*100:<15.2f} | {wer(truths, trt_preds)*100:<15.2f}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"🚀 TOTAL GPU SPEEDUP: {avg_pt/avg_trt:.2f}x faster\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_gpu_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQxD_DAZ5aw_",
        "outputId": "54780845-4919-487e-935f-9ec5094e3441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Original Model for Baseline...\n",
            "Loading Hindi test samples...\n",
            "Benchmarking 50 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "50it [00:08,  5.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "METRIC                    | PYTORCH (GPU)   | TENSORRT (FP16)\n",
            "------------------------------------------------------------\n",
            "Avg Latency (ms)          | 56.26           | 62.11          \n",
            "Word Error Rate (%)       | 35.39           | 113.20         \n",
            "============================================================\n",
            "🚀 TOTAL GPU SPEEDUP: 0.91x faster\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorrt as trt\n",
        "\n",
        "# Ensure NVIDIA libraries are visible\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/lib64-nvidia'\n",
        "\n",
        "def build_mixed_precision_engine():\n",
        "    logger = trt.Logger(trt.Logger.VERBOSE)\n",
        "    builder = trt.Builder(logger)\n",
        "\n",
        "    flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "    network = builder.create_network(flag)\n",
        "    config = builder.create_builder_config()\n",
        "    parser = trt.OnnxParser(network, logger)\n",
        "\n",
        "    # Use the SIMPLIFIED ONNX\n",
        "    onnx_path = \"model_optimized.onnx\"\n",
        "    print(f\"Parsing: {onnx_path}\")\n",
        "\n",
        "    with open(onnx_path, 'rb') as model:\n",
        "        if not parser.parse(model.read()):\n",
        "            print(\"❌ Parser Failed\")\n",
        "            return\n",
        "\n",
        "    # 1. Optimization Profile\n",
        "    profile = builder.create_optimization_profile()\n",
        "    input_name = network.get_input(0).name\n",
        "    profile.set_shape(input_name, (1, 16000), (1, 80000), (4, 160000))\n",
        "    config.add_optimization_profile(profile)\n",
        "\n",
        "    # 2. ENABLE FP16 GLOBALLY (The Speed)\n",
        "    if builder.platform_has_fast_fp16:\n",
        "        config.set_flag(trt.BuilderFlag.FP16)\n",
        "        print(\"🚀 FP16 Enabled (Global)\")\n",
        "\n",
        "    # 3. PROTECT LAYERS (The Accuracy Fix)\n",
        "    # We force specific unstable layers back to FP32\n",
        "    print(\"🛡️ Inspecting layers for mixed precision...\")\n",
        "    count_fp32 = 0\n",
        "\n",
        "    for i in range(network.num_layers):\n",
        "        layer = network.get_layer(i)\n",
        "\n",
        "        # Check for LayerNorm or large ElementWise operations\n",
        "        # These are usually the culprits for the 160% WER\n",
        "        if \"LayerNorm\" in layer.name or \"Pow\" in layer.name or \"ReduceMean\" in layer.name:\n",
        "            layer.precision = trt.DataType.FLOAT\n",
        "            # Ensure output is also Float32\n",
        "            if layer.num_outputs > 0:\n",
        "                layer.set_output_type(0, trt.DataType.FLOAT)\n",
        "            count_fp32 += 1\n",
        "\n",
        "    # 4. TRT 10 UPDATE: Use OBEY_PRECISION_CONSTRAINTS\n",
        "    # This forces TensorRT to respect the 'layer.precision' we just set\n",
        "    config.set_flag(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS)\n",
        "\n",
        "    print(f\"🔒 Protected {count_fp32} layers in FP32 mode.\")\n",
        "\n",
        "    # 5. Build\n",
        "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 * 1024 * 1024 * 1024)\n",
        "    print(\"Building Mixed-Precision Engine... (Wait 5-10 mins)\")\n",
        "\n",
        "    plan = builder.build_serialized_network(network, config)\n",
        "\n",
        "    if plan:\n",
        "        with open(\"model.engine\", \"wb\") as f:\n",
        "            f.write(plan)\n",
        "        print(\"✅ Success! Mixed-Precision Engine saved.\")\n",
        "    else:\n",
        "        print(\"❌ Build Failed. Check logs.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_mixed_precision_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d9Obo926Ynl",
        "outputId": "7994f76c-589f-4a23-b752-56d07bcb0fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing: model_optimized.onnx\n",
            "🚀 FP16 Enabled (Global)\n",
            "🛡️ Inspecting layers for mixed precision...\n",
            "🔒 Protected 171 layers in FP32 mode.\n",
            "Building Mixed-Precision Engine... (Wait 5-10 mins)\n",
            "✅ Success! Mixed-Precision Engine saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from datasets import load_dataset\n",
        "from jiwer import wer\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import io\n",
        "\n",
        "def run_gpu_benchmark():\n",
        "    engine_path = \"model.engine\"\n",
        "    model_id = \"ai4bharat/indicwav2vec-hindi\"\n",
        "\n",
        "    # 1. Setup TensorRT Runtime & Context\n",
        "    logger = trt.Logger(trt.Logger.INFO)\n",
        "    runtime = trt.Runtime(logger)\n",
        "    with open(engine_path, \"rb\") as f:\n",
        "        engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    context = engine.create_execution_context()\n",
        "\n",
        "    # 2. Setup Original Model for Baseline\n",
        "    # This uses your logged-in token automatically\n",
        "    print(\"Loading Original Model for Baseline...\")\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "    pt_model = Wav2Vec2ForCTC.from_pretrained(model_id).to(\"cuda\")\n",
        "    pt_model.eval()\n",
        "\n",
        "    # 3. Load Evaluation Data\n",
        "    print(\"Loading Hindi test samples...\")\n",
        "    # Using 'streaming=True' to avoid downloading the whole dataset at once\n",
        "    dataset = load_dataset(\"MatrixSpeechAI/Common_voice_hindi_denoised\", split=\"train\", streaming=True).decode(False)\n",
        "\n",
        "    trt_latencies, pt_latencies = [], []\n",
        "    trt_preds, pt_preds, truths = [], [], []\n",
        "\n",
        "    print(\"Benchmarking 50 samples...\")\n",
        "    for i, item in enumerate(tqdm(dataset.take(50))):\n",
        "        # Prepare Audio\n",
        "        audio, _ = librosa.load(io.BytesIO(item[\"audio\"][\"bytes\"]), sr=16000)\n",
        "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        input_values = inputs.input_values.to(\"cuda\")\n",
        "        input_np = input_values.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # --- PYTORCH BASELINE ---\n",
        "        torch.cuda.synchronize() # Ensure GPU is ready\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            pt_logits = pt_model(input_values).logits\n",
        "        torch.cuda.synchronize()\n",
        "        pt_latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "        pt_pred_ids = torch.argmax(pt_logits, dim=-1)\n",
        "        pt_preds.append(processor.batch_decode(pt_pred_ids)[0])\n",
        "\n",
        "        # --- TENSORRT INFERENCE ---\n",
        "        # 1. Set input shape for dynamic engine\n",
        "        context.set_input_shape(\"input_values\", input_np.shape)\n",
        "\n",
        "        # 2. Allocate memory on GPU\n",
        "        d_input = cuda.mem_alloc(input_np.nbytes)\n",
        "\n",
        "        # Wav2Vec2 typically reduces sequence length by 320x in the feature extractor\n",
        "        output_shape = (input_np.shape[0], input_np.shape[1] // 320, 108)\n",
        "        output_np = np.empty(output_shape, dtype=np.float32)\n",
        "        d_output = cuda.mem_alloc(output_np.nbytes)\n",
        "\n",
        "        # 3. Transfer, Execute, and Retrieve\n",
        "        cuda.memcpy_htod(d_input, input_np)\n",
        "\n",
        "        start = time.time()\n",
        "        context.execute_v2([int(d_input), int(d_output)])\n",
        "        cuda.memcpy_dtoh(output_np, d_output)\n",
        "        trt_latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "        trt_pred_ids = np.argmax(output_np, axis=-1)\n",
        "        trt_preds.append(processor.batch_decode(trt_pred_ids)[0])\n",
        "        truths.append(item[\"transcription\"])\n",
        "\n",
        "    # 4. REPORT\n",
        "    avg_pt = np.mean(pt_latencies)\n",
        "    avg_trt = np.mean(trt_latencies)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{'METRIC':<25} | {'PYTORCH (GPU)':<15} | {'TENSORRT ':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Avg Latency (ms)':<25} | {avg_pt:<15.2f} | {avg_trt:<15.2f}\")\n",
        "    print(f\"{'Word Error Rate (%)':<25} | {wer(truths, pt_preds)*100:<15.2f} | {wer(truths, trt_preds)*100:<15.2f}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"🚀 TOTAL GPU SPEEDUP: {avg_pt/avg_trt:.2f}x faster\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_gpu_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I776FrLH7U9I",
        "outputId": "e7f52f6b-833d-4786-b38b-9e54b91764f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Original Model for Baseline...\n",
            "Loading Hindi test samples...\n",
            "Benchmarking 50 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "50it [00:06,  7.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "METRIC                    | PYTORCH (GPU)   | TENSORRT       \n",
            "------------------------------------------------------------\n",
            "Avg Latency (ms)          | 60.17           | 17.09          \n",
            "Word Error Rate (%)       | 35.39           | 156.74         \n",
            "============================================================\n",
            "🚀 TOTAL GPU SPEEDUP: 3.52x faster\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorrt as trt\n",
        "\n",
        "# Ensure NVIDIA libraries are visible\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/lib64-nvidia'\n",
        "\n",
        "def build_relaxed_engine():\n",
        "    logger = trt.Logger(trt.Logger.VERBOSE)\n",
        "    builder = trt.Builder(logger)\n",
        "\n",
        "    flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "    network = builder.create_network(flag)\n",
        "    config = builder.create_builder_config()\n",
        "    parser = trt.OnnxParser(network, logger)\n",
        "\n",
        "    # 1. Parse the RAW ONNX\n",
        "    onnx_path = \"model.onnx\"\n",
        "    print(f\"Parsing Raw ONNX: {onnx_path}\")\n",
        "\n",
        "    with open(onnx_path, 'rb') as model:\n",
        "        if not parser.parse(model.read()):\n",
        "            print(\"❌ Parser Failed\")\n",
        "            return\n",
        "\n",
        "    # 2. Optimization Profile\n",
        "    profile = builder.create_optimization_profile()\n",
        "    input_name = network.get_input(0).name\n",
        "    profile.set_shape(input_name, (1, 16000), (1, 80000), (4, 160000))\n",
        "    config.add_optimization_profile(profile)\n",
        "\n",
        "    # 3. ENABLE FP16 (The Engine Power)\n",
        "    if builder.platform_has_fast_fp16:\n",
        "        config.set_flag(trt.BuilderFlag.FP16)\n",
        "        print(\"🚀 FP16 Enabled Globally (Speed Base)\")\n",
        "\n",
        "    # 4. THE FIX: Softly Hint FP32 for Sensitive Layers\n",
        "    # We tell TensorRT: \"Please try to keep these layers in FP32, but don't crash if you need to convert.\"\n",
        "    print(\"🛡️ Applying Soft Precision Constraints...\")\n",
        "    count_fp32 = 0\n",
        "\n",
        "    for i in range(network.num_layers):\n",
        "        layer = network.get_layer(i)\n",
        "\n",
        "        # Protect specific layer types known to break Wav2Vec2 accuracy\n",
        "        # LayerNorm usually consists of: ReduceMean, Pow, Sub, Div, Sqrt\n",
        "        if layer.type in [trt.LayerType.IDENTITY, trt.LayerType.REDUCE, trt.LayerType.ELEMENTWISE]:\n",
        "            # Setting precision here acts as a strong suggestion\n",
        "            layer.precision = trt.DataType.FLOAT\n",
        "            if layer.num_outputs > 0:\n",
        "                layer.set_output_type(0, trt.DataType.FLOAT)\n",
        "            count_fp32 += 1\n",
        "\n",
        "    # CRITICAL: We DO NOT set 'OBEY_PRECISION_CONSTRAINTS'.\n",
        "    # This lets TRT handle the FP16->FP32 conversions automatically.\n",
        "\n",
        "    print(f\"🔒 Hinted {count_fp32} layers to stay in FP32.\")\n",
        "\n",
        "    # 5. Build\n",
        "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 * 1024 * 1024 * 1024)\n",
        "    print(\"Building Relaxed Engine... (Wait 5-10 mins)\")\n",
        "\n",
        "    plan = builder.build_serialized_network(network, config)\n",
        "\n",
        "    if plan:\n",
        "        with open(\"model.engine\", \"wb\") as f:\n",
        "            f.write(plan)\n",
        "        print(\"✅ Success! Relaxed Mixed-Precision Engine saved.\")\n",
        "    else:\n",
        "        print(\"❌ Build Failed. Check logs above.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_relaxed_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBr1IcG78b1p",
        "outputId": "614592fa-8d94-41bb-8755-8aedd2286c77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing Raw ONNX: model.onnx\n",
            "🚀 FP16 Enabled Globally (Speed Base)\n",
            "🛡️ Applying Soft Precision Constraints...\n",
            "🔒 Hinted 841 layers to stay in FP32.\n",
            "Building Relaxed Engine... (Wait 5-10 mins)\n",
            "✅ Success! Relaxed Mixed-Precision Engine saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "from datasets import load_dataset\n",
        "from jiwer import wer\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import io\n",
        "\n",
        "def calculate_stats(truths, preds):\n",
        "    w = wer(truths, preds)\n",
        "    return w * 100\n",
        "\n",
        "def ctc_collapse(pred_ids, processor):\n",
        "    \"\"\"\n",
        "    Simulates the CTC decode logic: Collapse repeats and remove blanks.\n",
        "    \"\"\"\n",
        "    # 1. Group Repeats [A, A, B, B, _] -> [A, B, _]\n",
        "    grouped_ids = [x for i, x in enumerate(pred_ids) if i == 0 or x != pred_ids[i-1]]\n",
        "\n",
        "    # 2. Remove Blanks\n",
        "    blank_id = processor.tokenizer.pad_token_id\n",
        "    cleaned_ids = [x for x in grouped_ids if x != blank_id]\n",
        "\n",
        "    return processor.decode(cleaned_ids)\n",
        "\n",
        "def run_diagnostic():\n",
        "    engine_path = \"model.engine\"\n",
        "    model_id = \"ai4bharat/indicwav2vec-hindi\"\n",
        "\n",
        "    print(\"🔍 INITIALIZING DIAGNOSTIC TOOL...\")\n",
        "\n",
        "    # 1. Load Resources\n",
        "    logger = trt.Logger(trt.Logger.WARNING) # Reduce noise\n",
        "    with open(engine_path, \"rb\") as f, trt.Runtime(logger) as runtime:\n",
        "        engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    context = engine.create_execution_context()\n",
        "\n",
        "    print(\"   • TensorRT Engine Loaded\")\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "    print(\"   • Processor Loaded\")\n",
        "\n",
        "    # 2. Load Data (Just 10 samples for deep analysis)\n",
        "    dataset = load_dataset(\"MatrixSpeechAI/Common_voice_hindi_denoised\", split=\"train\", streaming=True).decode(False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"{'#':<3} | {'STATUS':<10} | {'TRANSCRIPTION ANALYSIS'}\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    trt_preds = []\n",
        "    truths = []\n",
        "    latencies = []\n",
        "\n",
        "    # 3. Analysis Loop\n",
        "    for i, item in enumerate(dataset.take(20)):\n",
        "        # Pre-process\n",
        "        audio, _ = librosa.load(io.BytesIO(item[\"audio\"][\"bytes\"]), sr=16000)\n",
        "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        input_np = inputs.input_values.numpy().astype(np.float32)\n",
        "\n",
        "        # TRT Inference\n",
        "        context.set_input_shape(\"input_values\", input_np.shape)\n",
        "        d_input = cuda.mem_alloc(input_np.nbytes)\n",
        "\n",
        "        seq_len = input_np.shape[1] // 320\n",
        "        output_shape = (1, seq_len, 108)\n",
        "        output_np = np.empty(output_shape, dtype=np.float32)\n",
        "        d_output = cuda.mem_alloc(output_np.nbytes)\n",
        "\n",
        "        cuda.memcpy_htod(d_input, input_np)\n",
        "        start = time.time()\n",
        "        context.execute_v2([int(d_input), int(d_output)])\n",
        "        cuda.memcpy_dtoh(output_np, d_output)\n",
        "        latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "        # --- DIAGNOSIS LOGIC ---\n",
        "        # 1. Get Raw IDs (Frame-by-frame)\n",
        "        raw_ids = np.argmax(output_np, axis=-1)[0]\n",
        "\n",
        "        # 2. Check for \"Garbage\" (Is it just repeating one token?)\n",
        "        unique_tokens = len(np.unique(raw_ids))\n",
        "        is_garbage = unique_tokens < 5 and len(raw_ids) > 50\n",
        "\n",
        "        # 3. Decode Clean\n",
        "        pred_text = ctc_collapse(raw_ids, processor)\n",
        "        truth_text = item[\"transcription\"]\n",
        "\n",
        "        trt_preds.append(pred_text)\n",
        "        truths.append(truth_text)\n",
        "\n",
        "        # Print detailed analysis for the first 5 samples\n",
        "        if i < 5:\n",
        "            status = \"🔴 FAIL\" if is_garbage or wer([truth_text], [pred_text]) > 0.5 else \"🟢 PASS\"\n",
        "            print(f\"{i+1:<3} | {status:<10} | Truth: {truth_text}\")\n",
        "            print(f\"{'':<3} | {'':<10} | Pred : {pred_text}\")\n",
        "            if is_garbage:\n",
        "                print(f\"{'':<3} | {'':<10} | ⚠️  GARBAGE DETECTED: Model outputting repetitive/broken tokens.\")\n",
        "            print(\"-\" * 100)\n",
        "\n",
        "    # 4. Final Verdict\n",
        "    avg_wer = calculate_stats(truths, trt_preds)\n",
        "    avg_lat = np.mean(latencies)\n",
        "\n",
        "    print(\"\\n📊 FINAL DIAGNOSTIC REPORT\")\n",
        "    print(f\"   • Avg Latency : {avg_lat:.2f} ms\")\n",
        "    print(f\"   • Final WER   : {avg_wer:.2f}%\")\n",
        "\n",
        "    if avg_wer > 100:\n",
        "        print(\"\\n❌ CRITICAL FAILURE: WER > 100%\")\n",
        "        print(\"   Diagnosis: FP16 Overflow.\")\n",
        "        print(\"   Explanation: The 'LayerNorm' layers in your model are breaking under 16-bit precision.\")\n",
        "        print(\"   Solution: You must rebuild the engine using FP32 or TF32 (TensorFloat-32).\")\n",
        "    elif avg_wer > 40:\n",
        "        print(\"\\n⚠️  WARNING: High WER\")\n",
        "        print(\"   Diagnosis: CTC Decoding Issue.\")\n",
        "        print(\"   Solution: The ctc_collapse function above fixes this. Ensure it is used in production.\")\n",
        "    else:\n",
        "        print(\"\\n✅ SUCCESS: Model is healthy.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_diagnostic()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTwAQdkN92U0",
        "outputId": "521457fa-0864-4342-fbef-a3a92a5f276f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 INITIALIZING DIAGNOSTIC TOOL...\n",
            "   • TensorRT Engine Loaded\n",
            "   • Processor Loaded\n",
            "\n",
            "====================================================================================================\n",
            "#   | STATUS     | TRANSCRIPTION ANALYSIS\n",
            "====================================================================================================\n",
            "1   | 🔴 FAIL     | Truth: तुम्हारे पास तीन महीने बचे हैं।\n",
            "    |            | Pred : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथ<unk>ङहउपझीऐम डॅकवअथोङहउपझीऐम ड<unk>कवअकोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउइढृथपकझथभशिे्<unk> इऐळ<unk> ऋखवी<unk>र<unk>वऑसग<unk></s>भं<unk><s>र<unk>ढ</s>घ<unk>पूह<unk>ए<unk>ठॉ<unk>हउ<unk>ठदए\n",
            "----------------------------------------------------------------------------------------------------\n",
            "2   | 🔴 FAIL     | Truth: मैं खाती हूँ।\n",
            "    |            | Pred : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङह<unk>पझीऐम डॅकव<unk>थोङहउपझीऐम<unk>डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम <unk>द<unk>श\n",
            "----------------------------------------------------------------------------------------------------\n",
            "3   | 🔴 FAIL     | Truth: जब मैं छोटा था मैं हमेशा जल्दी सोकर उठा करता था।\n",
            "    |            | Pred : झीऐम डॅकवअथोङहउपझीऐम डॅ<unk>वअथोङहउप<unk>झीऐम डॅकवअथोङहउपझीऐम डॅकवअथो<unk>हउ<unk>झीऐम डॅकवअथोङहउप<unk>ीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम टय<unk>ऑ<unk>ौङख<unk>क ञ<unk>फतहड<unk> <unk>ष<unk>ञफ<unk>औम<unk>ढ़<unk>ः<unk>कधि<unk>ओॉ<unk>फ<unk>ञगऋढछलदो<unk>छभ<unk>षउज<unk>खौ<unk>अॉभ<unk>क\n",
            "----------------------------------------------------------------------------------------------------\n",
            "4   | 🔴 FAIL     | Truth: बाईं ओर को एक गुप्त रास्ता है।\n",
            "    |            | Pred : झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकव<unk>थोङहउपझीऐम डॅकवअथोङहउप<unk>झीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउप<unk>द<unk>शु्<unk>ू<s><unk>सझगउ</s><unk>ऐइ<unk>डूॅखः<unk>ो<unk>म<unk>बठै<unk>िोध\n",
            "----------------------------------------------------------------------------------------------------\n",
            "5   | 🔴 FAIL     | Truth: मुझे इस लड़की से नफ़रत है।\n",
            "    |            | Pred : झीऐम डॅकवअथोङहउपझीऐम डॅकव<unk>थोङ<unk>उप<unk>झीऐम डॅ<unk>वअ<unk>ोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथ<unk>हउपझीऐम डॅकवअथोङहउपझीऐम डॅकवअथोङहउपझीऐम डॅ<unk>वअ<unk>ोङहउपझीऐम डॅकवअथो\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "📊 FINAL DIAGNOSTIC REPORT\n",
            "   • Avg Latency : 17.25 ms\n",
            "   • Final WER   : 163.41%\n",
            "\n",
            "❌ CRITICAL FAILURE: WER > 100%\n",
            "   Diagnosis: FP16 Overflow.\n",
            "   Explanation: The 'LayerNorm' layers in your model are breaking under 16-bit precision.\n",
            "   Solution: You must rebuild the engine using FP32 or TF32 (TensorFloat-32).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorrt as trt\n",
        "\n",
        "# Ensure NVIDIA libraries are visible\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/lib64-nvidia'\n",
        "\n",
        "def build_job_saver_engine():\n",
        "    logger = trt.Logger(trt.Logger.VERBOSE)\n",
        "    builder = trt.Builder(logger)\n",
        "\n",
        "    flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "    network = builder.create_network(flag)\n",
        "    config = builder.create_builder_config()\n",
        "    parser = trt.OnnxParser(network, logger)\n",
        "\n",
        "    # 1. USE RAW ONNX (Most compatible source)\n",
        "    onnx_path = \"model.onnx\"\n",
        "    print(f\"Parsing: {onnx_path}\")\n",
        "\n",
        "    with open(onnx_path, 'rb') as model:\n",
        "        if not parser.parse(model.read()):\n",
        "            print(\"❌ Parser Failed\")\n",
        "            return\n",
        "\n",
        "    # 2. Optimization Profile\n",
        "    profile = builder.create_optimization_profile()\n",
        "    input_name = network.get_input(0).name\n",
        "    profile.set_shape(input_name, (1, 16000), (1, 80000), (4, 160000))\n",
        "    config.add_optimization_profile(profile)\n",
        "\n",
        "    # 3. ENABLE FP16 SUPPORT\n",
        "    if builder.platform_has_fast_fp16:\n",
        "        config.set_flag(trt.BuilderFlag.FP16)\n",
        "\n",
        "    # 4. THE STRICT HYBRID LOGIC\n",
        "    # We set the strict flag. This tells TensorRT: \"If I don't give you a permission, use FP32.\"\n",
        "    config.set_flag(trt.BuilderFlag.OBEY_PRECISION_CONSTRAINTS)\n",
        "\n",
        "    print(\"🛡️ Applying Strict Hybrid Policy...\")\n",
        "    print(\"   • Default: FP32 (Safe for LayerNorm/Softmax)\")\n",
        "    print(\"   • Override: FP16 (Fast for Conv/MatMul)\")\n",
        "\n",
        "    count_fp16 = 0\n",
        "    count_fp32 = 0\n",
        "\n",
        "    for i in range(network.num_layers):\n",
        "        layer = network.get_layer(i)\n",
        "\n",
        "        # Only whitelist the \"Heavy Math\" layers for Speed\n",
        "        if layer.type == trt.LayerType.CONVOLUTION or layer.type == trt.LayerType.MATRIX_MULTIPLY:\n",
        "            layer.precision = trt.DataType.HALF\n",
        "            # We must explicitly set output type to match precision for OBEY mode\n",
        "            if layer.num_outputs > 0:\n",
        "                layer.set_output_type(0, trt.DataType.HALF)\n",
        "            count_fp16 += 1\n",
        "        else:\n",
        "            # EVERYTHING ELSE defaults to FP32 automatically because of OBEY flag.\n",
        "            # We don't need to set it manually, but we count it.\n",
        "            count_fp32 += 1\n",
        "\n",
        "    print(f\"🔒 Configuration: {count_fp16} Fast Layers (FP16) | {count_fp32} Safe Layers (FP32)\")\n",
        "\n",
        "    # 5. Build\n",
        "    # We increase memory to ensure TRT can build the \"Format Conversion\" layers needed between FP16/FP32\n",
        "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 * 1024 * 1024 * 1024)\n",
        "    print(\"Building Hybrid Engine... (Wait 5-10 mins)\")\n",
        "\n",
        "    plan = builder.build_serialized_network(network, config)\n",
        "\n",
        "    if plan:\n",
        "        with open(\"model.engine\", \"wb\") as f:\n",
        "            f.write(plan)\n",
        "        print(\"✅ Success! Job Saver Engine created.\")\n",
        "    else:\n",
        "        print(\"❌ Build Failed. Copy the VERBOSE logs above and show me.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_job_saver_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhZ7rJCt_Ogg",
        "outputId": "908a6070-6c49-415e-b8b6-7bcc710272bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing: model.onnx\n",
            "🛡️ Applying Strict Hybrid Policy...\n",
            "   • Default: FP32 (Safe for LayerNorm/Softmax)\n",
            "   • Override: FP16 (Fast for Conv/MatMul)\n",
            "🔒 Configuration: 202 Fast Layers (FP16) | 3334 Safe Layers (FP32)\n",
            "Building Hybrid Engine... (Wait 5-10 mins)\n",
            "✅ Success! Job Saver Engine created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from datasets import load_dataset\n",
        "from jiwer import wer\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import io\n",
        "\n",
        "def run_gpu_benchmark():\n",
        "    engine_path = \"model.engine\"\n",
        "    model_id = \"ai4bharat/indicwav2vec-hindi\"\n",
        "\n",
        "    # 1. Setup TensorRT Runtime & Context\n",
        "    logger = trt.Logger(trt.Logger.INFO)\n",
        "    runtime = trt.Runtime(logger)\n",
        "    with open(engine_path, \"rb\") as f:\n",
        "        engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    context = engine.create_execution_context()\n",
        "\n",
        "    # 2. Setup Original Model for Baseline\n",
        "    # This uses your logged-in token automatically\n",
        "    print(\"Loading Original Model for Baseline...\")\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "    pt_model = Wav2Vec2ForCTC.from_pretrained(model_id).to(\"cuda\")\n",
        "    pt_model.eval()\n",
        "\n",
        "    # 3. Load Evaluation Data\n",
        "    print(\"Loading Hindi test samples...\")\n",
        "    # Using 'streaming=True' to avoid downloading the whole dataset at once\n",
        "    dataset = load_dataset(\"MatrixSpeechAI/Common_voice_hindi_denoised\", split=\"train\", streaming=True).decode(False)\n",
        "\n",
        "    trt_latencies, pt_latencies = [], []\n",
        "    trt_preds, pt_preds, truths = [], [], []\n",
        "\n",
        "    print(\"Benchmarking 50 samples...\")\n",
        "    for i, item in enumerate(tqdm(dataset.take(50))):\n",
        "        # Prepare Audio\n",
        "        audio, _ = librosa.load(io.BytesIO(item[\"audio\"][\"bytes\"]), sr=16000)\n",
        "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        input_values = inputs.input_values.to(\"cuda\")\n",
        "        input_np = input_values.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # --- PYTORCH BASELINE ---\n",
        "        torch.cuda.synchronize() # Ensure GPU is ready\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            pt_logits = pt_model(input_values).logits\n",
        "        torch.cuda.synchronize()\n",
        "        pt_latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "        pt_pred_ids = torch.argmax(pt_logits, dim=-1)\n",
        "        pt_preds.append(processor.batch_decode(pt_pred_ids)[0])\n",
        "\n",
        "        # --- TENSORRT INFERENCE ---\n",
        "        # 1. Set input shape for dynamic engine\n",
        "        context.set_input_shape(\"input_values\", input_np.shape)\n",
        "\n",
        "        # 2. Allocate memory on GPU\n",
        "        d_input = cuda.mem_alloc(input_np.nbytes)\n",
        "\n",
        "        # Wav2Vec2 typically reduces sequence length by 320x in the feature extractor\n",
        "        output_shape = (input_np.shape[0], input_np.shape[1] // 320, 108)\n",
        "        output_np = np.empty(output_shape, dtype=np.float32)\n",
        "        d_output = cuda.mem_alloc(output_np.nbytes)\n",
        "\n",
        "        # 3. Transfer, Execute, and Retrieve\n",
        "        cuda.memcpy_htod(d_input, input_np)\n",
        "\n",
        "        start = time.time()\n",
        "        context.execute_v2([int(d_input), int(d_output)])\n",
        "        cuda.memcpy_dtoh(output_np, d_output)\n",
        "        trt_latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "        trt_pred_ids = np.argmax(output_np, axis=-1)\n",
        "        trt_preds.append(processor.batch_decode(trt_pred_ids)[0])\n",
        "        truths.append(item[\"transcription\"])\n",
        "\n",
        "    # 4. REPORT\n",
        "    avg_pt = np.mean(pt_latencies)\n",
        "    avg_trt = np.mean(trt_latencies)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{'METRIC':<25} | {'PYTORCH (GPU)':<15} | {'TENSORRT ':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Avg Latency (ms)':<25} | {avg_pt:<15.2f} | {avg_trt:<15.2f}\")\n",
        "    print(f\"{'Word Error Rate (%)':<25} | {wer(truths, pt_preds)*100:<15.2f} | {wer(truths, trt_preds)*100:<15.2f}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"🚀 TOTAL GPU SPEEDUP: {avg_pt/avg_trt:.2f}x faster\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_gpu_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAdilRn5CUrs",
        "outputId": "9323548d-1d59-4c5b-8297-b6720fed59c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Original Model for Baseline...\n",
            "Loading Hindi test samples...\n",
            "Benchmarking 50 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: ce55113e-5e76-4437-bc02-173d58e43fb5)')' thrown while requesting GET https://huggingface.co/datasets/MatrixSpeechAI/Common_voice_hindi_denoised/resolve/f4455c36f4fd22827fd7e9a087da6c5e4d020478/data/train-00000-of-00009.parquet\n",
            "WARNING:huggingface_hub.utils._http:'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: ce55113e-5e76-4437-bc02-173d58e43fb5)')' thrown while requesting GET https://huggingface.co/datasets/MatrixSpeechAI/Common_voice_hindi_denoised/resolve/f4455c36f4fd22827fd7e9a087da6c5e4d020478/data/train-00000-of-00009.parquet\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "50it [00:09,  5.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "METRIC                    | PYTORCH (GPU)   | TENSORRT       \n",
            "------------------------------------------------------------\n",
            "Avg Latency (ms)          | 64.23           | 15.15          \n",
            "Word Error Rate (%)       | 35.39           | 162.08         \n",
            "============================================================\n",
            "🚀 TOTAL GPU SPEEDUP: 4.24x faster\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorrt as trt\n",
        "import os\n",
        "\n",
        "def build_tf32_engine():\n",
        "    logger = trt.Logger(trt.Logger.VERBOSE)\n",
        "    builder = trt.Builder(logger)\n",
        "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
        "    config = builder.create_builder_config()\n",
        "    parser = trt.OnnxParser(network, logger)\n",
        "\n",
        "    with open(\"model.onnx\", 'rb') as model:\n",
        "        parser.parse(model.read())\n",
        "\n",
        "    profile = builder.create_optimization_profile()\n",
        "    profile.set_shape(network.get_input(0).name, (1, 16000), (1, 80000), (4, 160000))\n",
        "    config.add_optimization_profile(profile)\n",
        "\n",
        "    # DISABLE FP16 explicitly to stop the overflow\n",
        "    config.clear_flag(trt.BuilderFlag.FP16)\n",
        "\n",
        "    # ENABLE TF32 (Fast & Safe)\n",
        "    config.set_flag(trt.BuilderFlag.TF32)\n",
        "    print(\"🚀 Building in TF32 Mode (Safe Speed)...\")\n",
        "\n",
        "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 * 1024**3)\n",
        "\n",
        "    with open(\"model.engine\", \"wb\") as f:\n",
        "        f.write(builder.build_serialized_network(network, config))\n",
        "    print(\"✅ TF32 Engine Saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_tf32_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcCzq_q_Caoj",
        "outputId": "391fd9ea-99db-4136-925f-a1e2ec66d910"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Building in TF32 Mode (Safe Speed)...\n",
            "✅ TF32 Engine Saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import torch\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from datasets import load_dataset\n",
        "from jiwer import wer\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import io\n",
        "\n",
        "def run_gpu_benchmark():\n",
        "    engine_path = \"model.engine\"\n",
        "    model_id = \"ai4bharat/indicwav2vec-hindi\"\n",
        "\n",
        "    # 1. Setup TensorRT Runtime & Context\n",
        "    logger = trt.Logger(trt.Logger.INFO)\n",
        "    runtime = trt.Runtime(logger)\n",
        "    with open(engine_path, \"rb\") as f:\n",
        "        engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    context = engine.create_execution_context()\n",
        "\n",
        "    # 2. Setup Original Model for Baseline\n",
        "    # This uses your logged-in token automatically\n",
        "    print(\"Loading Original Model for Baseline...\")\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "    pt_model = Wav2Vec2ForCTC.from_pretrained(model_id).to(\"cuda\")\n",
        "    pt_model.eval()\n",
        "\n",
        "    # 3. Load Evaluation Data\n",
        "    print(\"Loading Hindi test samples...\")\n",
        "    # Using 'streaming=True' to avoid downloading the whole dataset at once\n",
        "    dataset = load_dataset(\"MatrixSpeechAI/Common_voice_hindi_denoised\", split=\"train\", streaming=True).decode(False)\n",
        "\n",
        "    trt_latencies, pt_latencies = [], []\n",
        "    trt_preds, pt_preds, truths = [], [], []\n",
        "\n",
        "    print(\"Benchmarking 50 samples...\")\n",
        "    for i, item in enumerate(tqdm(dataset.take(50))):\n",
        "        # Prepare Audio\n",
        "        audio, _ = librosa.load(io.BytesIO(item[\"audio\"][\"bytes\"]), sr=16000)\n",
        "        inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\")\n",
        "        input_values = inputs.input_values.to(\"cuda\")\n",
        "        input_np = input_values.cpu().numpy().astype(np.float32)\n",
        "\n",
        "        # --- PYTORCH BASELINE ---\n",
        "        torch.cuda.synchronize() # Ensure GPU is ready\n",
        "        start = time.time()\n",
        "        with torch.no_grad():\n",
        "            pt_logits = pt_model(input_values).logits\n",
        "        torch.cuda.synchronize()\n",
        "        pt_latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "        pt_pred_ids = torch.argmax(pt_logits, dim=-1)\n",
        "        pt_preds.append(processor.batch_decode(pt_pred_ids)[0])\n",
        "\n",
        "        # --- TENSORRT INFERENCE ---\n",
        "        # 1. Set input shape for dynamic engine\n",
        "        context.set_input_shape(\"input_values\", input_np.shape)\n",
        "\n",
        "        # 2. Allocate memory on GPU\n",
        "        d_input = cuda.mem_alloc(input_np.nbytes)\n",
        "\n",
        "        # Wav2Vec2 typically reduces sequence length by 320x in the feature extractor\n",
        "        output_shape = (input_np.shape[0], input_np.shape[1] // 320, 108)\n",
        "        output_np = np.empty(output_shape, dtype=np.float32)\n",
        "        d_output = cuda.mem_alloc(output_np.nbytes)\n",
        "\n",
        "        # 3. Transfer, Execute, and Retrieve\n",
        "        cuda.memcpy_htod(d_input, input_np)\n",
        "\n",
        "        start = time.time()\n",
        "        context.execute_v2([int(d_input), int(d_output)])\n",
        "        cuda.memcpy_dtoh(output_np, d_output)\n",
        "        trt_latencies.append((time.time() - start) * 1000)\n",
        "\n",
        "        trt_pred_ids = np.argmax(output_np, axis=-1)\n",
        "        trt_preds.append(processor.batch_decode(trt_pred_ids)[0])\n",
        "        truths.append(item[\"transcription\"])\n",
        "\n",
        "    # 4. REPORT\n",
        "    avg_pt = np.mean(pt_latencies)\n",
        "    avg_trt = np.mean(trt_latencies)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\"{'METRIC':<25} | {'PYTORCH (GPU)':<15} | {'TENSORRT ':<15}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Avg Latency (ms)':<25} | {avg_pt:<15.2f} | {avg_trt:<15.2f}\")\n",
        "    print(f\"{'Word Error Rate (%)':<25} | {wer(truths, pt_preds)*100:<15.2f} | {wer(truths, trt_preds)*100:<15.2f}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"🚀 TOTAL GPU SPEEDUP: {avg_pt/avg_trt:.2f}x faster\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_gpu_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icZTvFnkEVEE",
        "outputId": "8ac10c0a-bed2-41e7-942a-861ce6fa16a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Original Model for Baseline...\n",
            "Loading Hindi test samples...\n",
            "Benchmarking 50 samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: bd1ddcc5-0ddc-45fe-ba75-40e557baa7b8)')' thrown while requesting GET https://huggingface.co/datasets/MatrixSpeechAI/Common_voice_hindi_denoised/resolve/f4455c36f4fd22827fd7e9a087da6c5e4d020478/data/train-00000-of-00009.parquet\n",
            "WARNING:huggingface_hub.utils._http:'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: bd1ddcc5-0ddc-45fe-ba75-40e557baa7b8)')' thrown while requesting GET https://huggingface.co/datasets/MatrixSpeechAI/Common_voice_hindi_denoised/resolve/f4455c36f4fd22827fd7e9a087da6c5e4d020478/data/train-00000-of-00009.parquet\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "50it [00:11,  4.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "METRIC                    | PYTORCH (GPU)   | TENSORRT       \n",
            "------------------------------------------------------------\n",
            "Avg Latency (ms)          | 65.15           | 63.59          \n",
            "Word Error Rate (%)       | 35.39           | 113.76         \n",
            "============================================================\n",
            "🚀 TOTAL GPU SPEEDUP: 1.02x faster\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colored polygraphy --extra-index-url https://pypi.ngc.nvidia.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfeOE8FsFeHl",
        "outputId": "9a394ffb-8580-475b-c28b-dd05d270990d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting colored\n",
            "  Downloading colored-2.3.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting polygraphy\n",
            "  Downloading polygraphy-0.49.26-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Downloading colored-2.3.1-py3-none-any.whl (19 kB)\n",
            "Downloading polygraphy-0.49.26-py2.py3-none-any.whl (372 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m372.8/372.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: polygraphy, colored\n",
            "Successfully installed colored-2.3.1 polygraphy-0.49.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We add the --trt-min/opt/max-shapes flags to fix the \"Negative Dimension\" crash\n",
        "!polygraphy run model.onnx \\\n",
        "    --trt \\\n",
        "    --fp16 \\\n",
        "    --save-engine model.engine \\\n",
        "    --trt-min-shapes input_values:[1,16000] \\\n",
        "    --trt-opt-shapes input_values:[1,80000] \\\n",
        "    --trt-max-shapes input_values:[4,160000] \\\n",
        "    --verbose"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f7IizfvFsd1",
        "outputId": "27beceba-7c5b-4fbd-d1dc-308029772816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;14m[I] RUNNING | Command: /usr/local/bin/polygraphy run model.onnx --trt --fp16 --save-engine model.engine --trt-min-shapes input_values:[1,16000] --trt-opt-shapes input_values:[1,80000] --trt-max-shapes input_values:[4,160000] --verbose\u001b[0m\n",
            "\u001b[38;5;13m[V] Loaded Module: polygraphy | Version: 0.49.26 | Path: ['/usr/local/lib/python3.12/dist-packages/polygraphy']\u001b[0m\n",
            "\u001b[38;5;13m[V] Loaded extension modules: []\u001b[0m\n",
            "[I] TF32 is disabled by default. Turn on TF32 for better performance with minor accuracy differences.\n",
            "\u001b[38;5;14m[I] trt-runner-N0-01/26/26-11:38:50     | Activating and starting inference\u001b[0m\n",
            "\u001b[38;5;13m[V] Loaded Module: tensorrt | Version: 10.0.1 | Path: ['/usr/local/lib/python3.12/dist-packages/tensorrt']\u001b[0m\n",
            "\u001b[38;5;13m[V] [MemUsageChange] Init CUDA: CPU +17, GPU +0, now: CPU 26, GPU 3106 (MiB)\u001b[0m\n",
            "\u001b[38;5;13m[V] [MemUsageChange] Init builder kernel library: CPU +953, GPU +180, now: CPU 1115, GPU 3286 (MiB)\u001b[0m\n",
            "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:604] Reading dangerously large protocol message.  If the message turns out to be larger than 2147483647 bytes, parsing will be halted for security reasons.  To increase the limit (or to disable these warnings), see CodedInputStream::SetTotalBytesLimit() in google/protobuf/io/coded_stream.h.\n",
            "[libprotobuf WARNING google/protobuf/io/coded_stream.cc:81] The total number of bytes read was 1262540130\n",
            "\u001b[38;5;13m[V] ----------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;13m[V] Input filename:   /content/model.onnx\u001b[0m\n",
            "\u001b[38;5;13m[V] ONNX IR version:  0.0.7\u001b[0m\n",
            "\u001b[38;5;13m[V] Opset version:    14\u001b[0m\n",
            "\u001b[38;5;13m[V] Producer name:    pytorch\u001b[0m\n",
            "\u001b[38;5;13m[V] Producer version: 2.10.0\u001b[0m\n",
            "\u001b[38;5;13m[V] Domain:           \u001b[0m\n",
            "\u001b[38;5;13m[V] Model version:    0\u001b[0m\n",
            "\u001b[38;5;13m[V] Doc string:       \u001b[0m\n",
            "\u001b[38;5;13m[V] ----------------------------------------------------------------\u001b[0m\n",
            "\u001b[38;5;13m[V] Setting TensorRT Optimization Profiles\u001b[0m\n",
            "\u001b[38;5;13m[V] Input tensor: input_values (dtype=DataType.FLOAT, shape=(-1, -1)) | Setting input tensor shapes to: (min=[1, 16000], opt=[1, 80000], max=[4, 160000])\u001b[0m\n",
            "[I] Configuring with profiles:[\n",
            "        Profile 0:\n",
            "            {input_values [min=[1, 16000], opt=[1, 80000], max=[4, 160000]]}\n",
            "    ]\n",
            "\u001b[38;5;11m[W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\u001b[0m\n",
            "\u001b[38;5;14m[I] Building engine with configuration:\n",
            "    Flags                  | [FP16]\n",
            "    Engine Capability      | EngineCapability.STANDARD\n",
            "    Memory Pools           | [WORKSPACE: 15095.06 MiB, TACTIC_DRAM: 15095.06 MiB, TACTIC_SHARED_MEMORY: 1024.00 MiB]\n",
            "    Tactic Sources         | [EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
            "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
            "    Preview Features       | [PROFILE_SHARING_0806]\u001b[0m\n",
            "\u001b[38;5;11m[W] Detected layernorm nodes in FP16.\u001b[0m\n",
            "\u001b[38;5;11m[W] Running layernorm after self-attention in FP16 may cause overflow. Exporting the model to the latest available ONNX opset (later than opset 17) to use the INormalizationLayer, or forcing layernorm layers to run in FP32 precision can help with preserving accuracy.\u001b[0m\n",
            "\u001b[38;5;13m[V] Global timing cache in use. Profiling results in this builder pass will be stored.\u001b[0m\n",
            "\u001b[38;5;13m[V] Detected 1 inputs and 1 output network tensors.\u001b[0m\n",
            "\u001b[38;5;13m[V] Total Host Persistent Memory: 38080\u001b[0m\n",
            "\u001b[38;5;13m[V] Total Device Persistent Memory: 372736\u001b[0m\n",
            "\u001b[38;5;13m[V] Total Scratch Memory: 262391808\u001b[0m\n",
            "\u001b[38;5;13m[V] [BlockAssignment] Started assigning block shifts. This will take 53 steps to complete.\u001b[0m\n",
            "\u001b[38;5;13m[V] [BlockAssignment] Algorithm ShiftNTopDown took 1.16267ms to assign 5 blocks to 53 nodes requiring 405859840 bytes.\u001b[0m\n",
            "\u001b[38;5;13m[V] Total Activation Memory: 405859328\u001b[0m\n",
            "\u001b[38;5;13m[V] Total Weights Memory: 631021056\u001b[0m\n",
            "\u001b[38;5;13m[V] Engine generation completed in 84.5262 seconds.\u001b[0m\n",
            "\u001b[38;5;13m[V] [MemUsageStats] Peak memory usage of TRT CPU/GPU memory allocators: CPU 577 MiB, GPU 1346 MiB\u001b[0m\n",
            "\u001b[38;5;13m[V] [MemUsageStats] Peak memory usage during Engine building and serialization: CPU: 10207 MiB\u001b[0m\n",
            "\u001b[38;5;10m[I] Finished engine building in 88.514 seconds\u001b[0m\n",
            "\u001b[38;5;13m[V] Loaded engine size: 607 MiB\u001b[0m\n",
            "\u001b[38;5;13m[V] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +387, now: CPU 0, GPU 989 (MiB)\u001b[0m\n",
            "\u001b[38;5;13m[V] Found candidate CUDA libraries: ['/usr/local/cuda/lib64/libcudart.so.12.5.82', '/usr/local/cuda/lib64/libcudart.so.12', '/usr/local/cuda/lib64/libcudart.so']\u001b[0m\n",
            "\u001b[38;5;13m[V] Loading inputs from data loader\u001b[0m\n",
            "\u001b[38;5;13m[V] Generating data using numpy seed: 1\u001b[0m\n",
            "\u001b[38;5;13m[V] Loaded Module: numpy | Version: 2.0.2 | Path: ['/usr/local/lib/python3.12/dist-packages/numpy']\u001b[0m\n",
            "\u001b[38;5;11m[W] Input tensor: input_values [shape=BoundedShape([-1, -1], min=None, max=None)] | Will generate data of shape: [1, 1].\n",
            "    If this is incorrect, please provide a custom data loader.\u001b[0m\n",
            "\u001b[38;5;13m[V] Input tensor: input_values | Generating input data in range: [0.0, 1.0]\u001b[0m\n",
            "[I] trt-runner-N0-01/26/26-11:38:50    \n",
            "    ---- Inference Input(s) ----\n",
            "    {input_values [dtype=float32, shape=(1, 1)]}\n",
            "\u001b[38;5;13m[V] trt-runner-N0-01/26/26-11:38:50     | Input metadata is: {input_values [dtype=float32, shape=(-1, -1)]}\u001b[0m\n",
            "\u001b[38;5;13m[V] Loaded Module: torch | Version: 2.9.0+cu126 | Path: ['/usr/local/lib/python3.12/dist-packages/torch']\u001b[0m\n",
            "\u001b[38;5;9m[E] 3: [executionContext.cpp::setInputShape::2068] Error Code 3: API Usage Error (Parameter check failed at: runtime/api/executionContext.cpp::setInputShape::2068, condition: satisfyProfile Runtime dimension does not satisfy any optimization profile.)\u001b[0m\n",
            "\u001b[38;5;9m[!] For input: input_values, failed to set shape to: (1, 1)\u001b[0m\n",
            "\u001b[38;5;9m[E] FAILED | Runtime: 162.934s | Command: /usr/local/bin/polygraphy run model.onnx --trt --fp16 --save-engine model.engine --trt-min-shapes input_values:[1,16000] --trt-opt-shapes input_values:[1,80000] --trt-max-shapes input_values:[4,160000] --verbose\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorrt as trt\n",
        "\n",
        "os.environ['LD_LIBRARY_PATH'] += ':/usr/lib64-nvidia'\n",
        "\n",
        "def build_stable_engine():\n",
        "    logger = trt.Logger(trt.Logger.VERBOSE)\n",
        "    builder = trt.Builder(logger)\n",
        "\n",
        "    flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
        "    network = builder.create_network(flag)\n",
        "    config = builder.create_builder_config()\n",
        "    parser = trt.OnnxParser(network, logger)\n",
        "\n",
        "    # 1. Parse RAW ONNX (Most robust source)\n",
        "    onnx_path = \"model.onnx\"\n",
        "    print(f\"Parsing: {onnx_path}\")\n",
        "    with open(onnx_path, 'rb') as model:\n",
        "        if not parser.parse(model.read()):\n",
        "            print(\"❌ Parser Failed\")\n",
        "            return\n",
        "\n",
        "    # 2. Optimization Profile\n",
        "    # TIGHTEN the profile to improve speed\n",
        "    profile = builder.create_optimization_profile()\n",
        "    # Min: 0.5s, Opt: 3s, Max: 8s (Most samples are short, this speeds up inference)\n",
        "    profile.set_shape(\"input_values\", (1, 8000), (1, 48000), (1, 128000))\n",
        "    config.add_optimization_profile(profile)\n",
        "\n",
        "    # 3. FORCE FP32 (STABILITY)\n",
        "    # We explicitly do NOT set the FP16 flag.\n",
        "    # This guarantees the WER will match PyTorch (35%).\n",
        "    print(\"🛡️ Building in Stable FP32 Mode...\")\n",
        "\n",
        "    # 4. Build\n",
        "    config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 4 * 1024**3)\n",
        "\n",
        "    plan = builder.build_serialized_network(network, config)\n",
        "    if plan:\n",
        "        with open(\"model.engine\", \"wb\") as f:\n",
        "            f.write(plan)\n",
        "        print(\"✅ Success! Stable Engine Saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    build_stable_engine()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MvQZ22lHLoB",
        "outputId": "d282e124-bcf4-4909-848a-e500ff1c2bdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing: model.onnx\n",
            "🛡️ Building in Stable FP32 Mode...\n",
            "✅ Success! Stable Engine Saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import tensorrt as trt\n",
        "import pycuda.driver as cuda\n",
        "import pycuda.autoinit\n",
        "import torch\n",
        "from transformers import Wav2Vec2Processor\n",
        "from datasets import load_dataset\n",
        "from jiwer import wer\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "import io\n",
        "\n",
        "def run_final_benchmark():\n",
        "    engine_path = \"model.engine\"\n",
        "    model_id = \"ai4bharat/indicwav2vec-hindi\"\n",
        "\n",
        "    # Setup\n",
        "    logger = trt.Logger(trt.Logger.WARNING)\n",
        "    with open(engine_path, \"rb\") as f, trt.Runtime(logger) as runtime:\n",
        "        engine = runtime.deserialize_cuda_engine(f.read())\n",
        "    context = engine.create_execution_context()\n",
        "\n",
        "    processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
        "    dataset = load_dataset(\"MatrixSpeechAI/Common_voice_hindi_denoised\", split=\"train\", streaming=True).decode(False)\n",
        "\n",
        "    # Warmup (Critical for fair speed comparison)\n",
        "    print(\"🔥 Warming up GPU...\")\n",
        "    dummy_input = np.zeros((1, 48000), dtype=np.float32)\n",
        "    d_dummy = cuda.mem_alloc(dummy_input.nbytes)\n",
        "    d_out_dummy = cuda.mem_alloc(dummy_input.nbytes) # Size doesn't matter for warmup\n",
        "    context.set_input_shape(\"input_values\", dummy_input.shape)\n",
        "    for _ in range(10):\n",
        "        context.execute_v2([int(d_dummy), int(d_out_dummy)])\n",
        "\n",
        "    trt_preds, truths = [], []\n",
        "    trt_times = []\n",
        "\n",
        "    print(\"🚀 Benchmarking...\")\n",
        "    for i, item in enumerate(tqdm(dataset.take(50))):\n",
        "        audio, _ = librosa.load(io.BytesIO(item[\"audio\"][\"bytes\"]), sr=16000)\n",
        "\n",
        "        # TensorRT Inference\n",
        "        input_np = processor(audio, sampling_rate=16000, return_tensors=\"np\").input_values\n",
        "        context.set_input_shape(\"input_values\", input_np.shape)\n",
        "\n",
        "        d_input = cuda.mem_alloc(input_np.nbytes)\n",
        "        output_shape = (1, input_np.shape[1] // 320, 108)\n",
        "        output_np = np.empty(output_shape, dtype=np.float32)\n",
        "        d_output = cuda.mem_alloc(output_np.nbytes)\n",
        "\n",
        "        cuda.memcpy_htod(d_input, input_np)\n",
        "\n",
        "        # Sync before time start\n",
        "        cuda.Context.synchronize()\n",
        "        start = time.time()\n",
        "        context.execute_v2([int(d_input), int(d_output)])\n",
        "        cuda.Context.synchronize() # Sync after execution\n",
        "        end = time.time()\n",
        "\n",
        "        trt_times.append((end - start) * 1000)\n",
        "        cuda.memcpy_dtoh(output_np, d_output)\n",
        "\n",
        "        # Decode\n",
        "        pred_ids = np.argmax(output_np, axis=-1)[0]\n",
        "        grouped_ids = [x for i, x in enumerate(pred_ids) if i == 0 or x != pred_ids[i-1]]\n",
        "        clean_ids = [x for x in grouped_ids if x != processor.tokenizer.pad_token_id]\n",
        "        trt_preds.append(processor.decode(clean_ids))\n",
        "        truths.append(item[\"transcription\"])\n",
        "\n",
        "    avg_lat = np.mean(trt_times)\n",
        "    final_wer = wer(truths, trt_preds) * 100\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"Final WER      : {final_wer:.2f}% (Target: ~35%)\")\n",
        "    print(f\"Avg Latency    : {avg_lat:.2f} ms\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_final_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVSbdq2vFwsJ",
        "outputId": "2ad393d9-0810-4794-f0f6-10bddc80759e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔥 Warming up GPU...\n",
            "🚀 Benchmarking...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "50it [00:06,  7.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Final WER      : 115.17% (Target: ~35%)\n",
            "Avg Latency    : 62.42 ms\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQcN3s-fHCSF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}